{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae60e4ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-bb577f25a9d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-bb577f25a9d1>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[0mplot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     rewards = n_step_Q(n_timesteps, max_episode_length, learning_rate, gamma, \n\u001b[0m\u001b[0;32m    112\u001b[0m                    policy, epsilon, temp, plot, n=n)\n\u001b[0;32m    113\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Obtained rewards: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-bb577f25a9d1>\u001b[0m in \u001b[0;36mn_step_Q\u001b[1;34m(n_timesteps, max_episode_length, learning_rate, gamma, policy, epsilon, temp, plot, n)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms_next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                 \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_sa\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_sa\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplot_optimal_policy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep_pause\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Plot the Q-value estimates during n-step Q-learning execution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstates_ep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions_ep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards_ep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\RL Assignemnt1\\Environment.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, Q_sa, plot_optimal_policy, step_pause)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;31m# Draw figure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpause\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_pause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_state_to_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mpause\u001b[1;34m(interval)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Practical for course 'Reinforcement Learning',\n",
    "Leiden University, The Netherlands\n",
    "2021\n",
    "By Thomas Moerland\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from Environment import StochasticWindyGridworld\n",
    "from Helper import softmax, argmax\n",
    "\n",
    "class NstepQLearningAgent:\n",
    "\n",
    "    def __init__(self, n_states, n_actions, learning_rate, gamma, n):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.n = n\n",
    "        self.Q_sa = np.zeros((n_states,n_actions))\n",
    "        \n",
    "    def select_action(self, s, policy='egreedy', epsilon=None, temp=None):\n",
    "        \n",
    "        actions = list(range(self.n_actions))\n",
    "        \n",
    "        if policy == 'egreedy':\n",
    "            if epsilon is None:\n",
    "                raise KeyError(\"Provide an epsilon\")\n",
    "            b = argmax([self.Q_sa[s,b] for b in range(self.n_actions)])\n",
    "            actions.append(b)\n",
    "            a = np.random.choice(actions,p=[epsilon/self.n_actions for i in range(self.n_actions)] + [1 - epsilon])\n",
    "                \n",
    "        elif policy == 'softmax':\n",
    "            if temp is None:\n",
    "                raise KeyError(\"Provide a temperature\")\n",
    "            probabilities = softmax(np.array(self.Q_sa[s,:]), temp)\n",
    "            a = np.random.choice(actions, p=probabilities)\n",
    "            \n",
    "        return a\n",
    "        \n",
    "    def update(self, states, actions, rewards, done):\n",
    "        ''' states is a list of states observed in the episode, of length T_ep + 1 (last state is appended)\n",
    "        actions is a list of actions observed in the episode, of length T_ep\n",
    "        rewards is a list of rewards observed in the episode, of length T_ep\n",
    "        done indicates whether the final s in states is a terminal state '''\n",
    "        T_ep = len(states)\n",
    "        for i in range(T_ep-1):\n",
    "            m = np.min([self.n, T_ep - i-1])\n",
    "            if done and i+m == T_ep:\n",
    "                g = np.sum([gamma**j*rewards[i+j] for j in range(m)])\n",
    "                self.Q_sa[states[i],actions[i]] += self.learning_rate*(g - self.Q_sa[states_ep[i],actions_ep[i]])\n",
    "            else:\n",
    "                g = np.sum([self.gamma**j*rewards[i+j] for j in range(m)]) + self.gamma**m*np.max([self.Q_sa[states[i+m],a] for a in range(self.n_actions)])\n",
    "                self.Q_sa[states[i],actions[i]] += self.learning_rate*(g - self.Q_sa[states[i],actions[i]])        \n",
    "        \n",
    "        \n",
    "def n_step_Q(n_timesteps, max_episode_length, learning_rate, gamma, \n",
    "                   policy='egreedy', epsilon=None, temp=None, plot=True, n=5):\n",
    "    ''' runs a single repetition of an MC rl agent\n",
    "    Return: rewards, a vector with the observed rewards at each timestep ''' \n",
    "    \n",
    "    env = StochasticWindyGridworld(initialize_model=False)\n",
    "    pi = NstepQLearningAgent(env.n_states, env.n_actions, learning_rate, gamma, n)\n",
    "    rewards = []\n",
    "    budget = n_timesteps\n",
    "    \n",
    "    while budget:\n",
    "        s = env.reset()\n",
    "        t = -1\n",
    "        rewards_ep = []\n",
    "        actions_ep = []\n",
    "        states_ep = [s]\n",
    "        for i in range(max_episode_length):  #Collect episode\n",
    "            budget = budget - 1\n",
    "            t += 1\n",
    "            a = pi.select_action(s, policy = policy, epsilon = epsilon, temp = temp)\n",
    "            s_next, r, done = env.step(a)\n",
    "            rewards.append(r)\n",
    "            rewards_ep.append(r)\n",
    "            actions_ep.append(a)\n",
    "            states_ep.append(s_next)\n",
    "            if done:\n",
    "                break\n",
    "            if not budget:\n",
    "                break\n",
    "            s = s_next\n",
    "            if plot:\n",
    "                env.render(Q_sa=pi.Q_sa,plot_optimal_policy=True,step_pause=0.01) # Plot the Q-value estimates during n-step Q-learning execution\n",
    "\n",
    "        pi.update(states = states_ep, actions = actions_ep, rewards = rewards_ep, done = done)\n",
    "        \n",
    "    return rewards \n",
    "\n",
    "def test():\n",
    "    n_timesteps = 10000\n",
    "    max_episode_length = 100\n",
    "    gamma = 1.0\n",
    "    learning_rate = 0.1\n",
    "    n = 3\n",
    "    \n",
    "    # Exploration\n",
    "    policy = 'softmax' # 'egreedy' or 'softmax' \n",
    "    epsilon = 0.1\n",
    "    temp = 1.0\n",
    "    \n",
    "    # Plotting parameters\n",
    "    plot = True\n",
    "\n",
    "    rewards = n_step_Q(n_timesteps, max_episode_length, learning_rate, gamma, \n",
    "                   policy, epsilon, temp, plot, n=n)\n",
    "    print(\"Obtained rewards: {}\".format(rewards))    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab331b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
