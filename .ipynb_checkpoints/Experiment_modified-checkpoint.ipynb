{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c807bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running one setting takes 5.719555620352427 minutes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-23d50601ca12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-23d50601ca12>\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mepsilons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         learning_curve = average_over_repetitions(backup, n_repetitions, n_timesteps, max_episode_length, learning_rate, \n\u001b[0m\u001b[0;32m     94\u001b[0m                                               gamma, policy, epsilon, temp, smoothing_window, plot, n)\n\u001b[0;32m     95\u001b[0m         \u001b[0mPlot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_curve\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr'$\\epsilon$-greedy, $\\epsilon $ = {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-23d50601ca12>\u001b[0m in \u001b[0;36maverage_over_repetitions\u001b[1;34m(backup, n_repetitions, n_timesteps, max_episode_length, learning_rate, gamma, policy, epsilon, temp, smoothing_window, plot, n)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_repetitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Loop over repetitions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbackup\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'q'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mbackup\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sarsa'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msarsa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\RL Assignemnt1\\Q_learning.py\u001b[0m in \u001b[0;36mq_learning\u001b[1;34m(n_timesteps, learning_rate, gamma, policy, epsilon, temp, plot)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0ms_next\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_next\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\RL Assignemnt1\\Q_learning.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, s, a, r, s_next, done)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms_next\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_sa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms_next\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_sa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_sa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2703\u001b[0m     \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2704\u001b[0m     \"\"\"\n\u001b[1;32m-> 2705\u001b[1;33m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[0;32m   2706\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0;32m   2707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Practical for course 'Reinforcement Learning',\n",
    "Leiden University, The Netherlands\n",
    "2021\n",
    "By Thomas Moerland\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg') # or TkAgg\n",
    "\n",
    "from Q_learning import q_learning, QLearningAgent\n",
    "from SARSA import sarsa\n",
    "from Environment import StochasticWindyGridworld\n",
    "from MonteCarlo import monte_carlo\n",
    "from Nstep import n_step_Q\n",
    "from Helper import LearningCurvePlot, smooth\n",
    "\n",
    "def average_over_repetitions(backup, n_repetitions, n_timesteps, max_episode_length, learning_rate, gamma, policy='egreedy', \n",
    "                    epsilon=None, temp=None, smoothing_window=51, plot=False, n=5):\n",
    "\n",
    "    reward_results = np.empty([n_repetitions,n_timesteps]) # Result array\n",
    "    now = time.time()\n",
    "    \n",
    "    for rep in range(n_repetitions): # Loop over repetitions\n",
    "        if backup == 'q':\n",
    "            rewards = q_learning(n_timesteps, learning_rate, gamma, policy, epsilon, temp, plot)\n",
    "        elif backup == 'sarsa':\n",
    "            rewards = sarsa(n_timesteps, learning_rate, gamma, policy, epsilon, temp, plot)\n",
    "        elif backup == 'mc':\n",
    "            rewards = monte_carlo(n_timesteps, max_episode_length, learning_rate, gamma, \n",
    "                   policy, epsilon, temp, plot)\n",
    "        elif backup == 'nstep':\n",
    "            rewards = n_step_Q(n_timesteps, max_episode_length, learning_rate, gamma, \n",
    "                   policy, epsilon, temp, plot, n=n)\n",
    "\n",
    "        reward_results[rep] = rewards\n",
    "        \n",
    "    print('Running one setting takes {} minutes'.format((time.time()-now)/60))    \n",
    "    learning_curve = np.mean(reward_results,axis=0) # average over repetitions\n",
    "    learning_curve = smooth(learning_curve,smoothing_window) # additional smoothing\n",
    "    return learning_curve  \n",
    "\n",
    "def experiment():\n",
    "    ####### Settings\n",
    "    # Experiment    \n",
    "    n_repetitions = 50\n",
    "    smoothing_window = 1001\n",
    "\n",
    "    # MDP    \n",
    "    n_timesteps = 50000\n",
    "    max_episode_length = 100\n",
    "    gamma = 1.0\n",
    "\n",
    "    # Exploration\n",
    "    policy = 'egreedy' # 'egreedy' or 'softmax' \n",
    "    epsilon = 0.05\n",
    "    temp = 1.0\n",
    "    \n",
    "    # Target and update\n",
    "    backup = 'q' # 'q' or 'sarsa' or 'mc' or 'nstep'\n",
    "    learning_rate = 0.25\n",
    "    n = 5\n",
    "        \n",
    "    # Plotting parameters\n",
    "    plot = False\n",
    "    \n",
    "    # Nice labels for plotting\n",
    "    policy_labels = {'egreedy': '$\\epsilon$-greedy policy',\n",
    "                  'softmax': 'Softmax policy'}\n",
    "\n",
    "    backup_labels = {'q': 'Q-learning',\n",
    "                  'sarsa': 'SARSA',\n",
    "                  'mc': 'Monte Carlo',\n",
    "                  'nstep': 'n-step Q-learning'}\n",
    "    \n",
    "    ####### Experiments\n",
    "    \n",
    "    #### Assignment 1: Dynamic Programming\n",
    "    # Execute this assignment in DynamicProgramming.py\n",
    "    optimal_average_reward_per_timestep = 1.11 # set the optimal average reward per timestep you found in the DP assignment here\n",
    "    \n",
    "    #### Assignment 2: Effect of exploration\n",
    "    backup = 'q'\n",
    "    Plot = LearningCurvePlot(title = 'Q-learning: effect of $\\epsilon$-greedy versus softmax exploration')    \n",
    "    policy = 'egreedy'\n",
    "    epsilons = [0.01,0.05,0.2]\n",
    "    for epsilon in epsilons:        \n",
    "        learning_curve = average_over_repetitions(backup, n_repetitions, n_timesteps, max_episode_length, learning_rate, \n",
    "                                              gamma, policy, epsilon, temp, smoothing_window, plot, n)\n",
    "        Plot.add_curve(learning_curve,label=r'$\\epsilon$-greedy, $\\epsilon $ = {}'.format(epsilon))    \n",
    "    policy = 'softmax'\n",
    "    temps = [0.01,0.1,1.0]\n",
    "    for temp in temps:\n",
    "        learning_curve = average_over_repetitions(backup, n_repetitions, n_timesteps, max_episode_length, learning_rate, \n",
    "                                              gamma, policy, epsilon, temp, smoothing_window, plot, n)\n",
    "        Plot.add_curve(learning_curve,label=r'softmax, $ \\tau $ = {}'.format(temp))\n",
    "    Plot.add_hline(optimal_average_reward_per_timestep, label=\"DP optimum\")\n",
    "    Plot.save('exploration.png')\n",
    "    policy = 'egreedy'\n",
    "    epsilon = 0.05 # set epsilon back to original value \n",
    "    temp = 1.0\n",
    "    \n",
    "    \n",
    "    ###### Assignment 3: Q-learning versus SARSA\n",
    "    backups = ['q','sarsa']\n",
    "    learning_rates = [0.05,0.2,0.4]\n",
    "    Plot = LearningCurvePlot(title = 'Q-learning versus SARSA')    \n",
    "    for backup in backups:\n",
    "        for learning_rate in learning_rates:\n",
    "            learning_curve = average_over_repetitions(backup, n_repetitions, n_timesteps, max_episode_length, learning_rate, \n",
    "                                                  gamma, policy, epsilon, temp, smoothing_window, plot, n)\n",
    "            Plot.add_curve(learning_curve,label=r'{}, $\\alpha$ = {} '.format(backup_labels[backup],learning_rate))\n",
    "    Plot.add_hline(optimal_average_reward_per_timestep, label=\"DP optimum\")\n",
    "    Plot.save('on_off_policy.png')\n",
    "    # Set back to original values\n",
    "    learning_rate = 0.25\n",
    "    backup = 'q'\n",
    "\n",
    "\n",
    "    # ##### Assignment 4: Back-up depth\n",
    "    backup = 'nstep'\n",
    "    ns = [1,3,5,10,20,100]\n",
    "    Plot = LearningCurvePlot(title = 'Effect of target depth')    \n",
    "    for n in ns:\n",
    "        learning_curve = average_over_repetitions(backup, n_repetitions, n_timesteps, max_episode_length, learning_rate, \n",
    "                                              gamma, policy, epsilon, temp, smoothing_window, plot, n)\n",
    "        Plot.add_curve(learning_curve,label=r'{}-step Q-learning'.format(n))\n",
    "    backup = 'mc'\n",
    "    learning_curve = average_over_repetitions(backup, n_repetitions, n_timesteps, max_episode_length, learning_rate, \n",
    "                                          gamma, policy, epsilon, temp, smoothing_window, plot, n)\n",
    "    Plot.add_curve(learning_curve,label='Monte Carlo')        \n",
    "    Plot.add_hline(optimal_average_reward_per_timestep, label=\"DP optimum\")\n",
    "    Plot.save('depth.png')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "867df36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running one setting takes 6.317164580027263 minutes\n",
      "Running one setting takes 6.068985962867737 minutes\n",
      "Running one setting takes 6.057094184557597 minutes\n",
      "Running one setting takes 6.000495227177938 minutes\n",
      "Running one setting takes 6.143618416786194 minutes\n",
      "Running one setting takes 6.375842312971751 minutes\n",
      "Running one setting takes 5.480473244190216 minutes\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Practical for course 'Reinforcement Learning',\n",
    "Leiden University, The Netherlands\n",
    "2021\n",
    "By Thomas Moerland\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg') # or TkAgg\n",
    "\n",
    "from Q_learning import q_learning, QLearningAgent\n",
    "from SARSA import sarsa\n",
    "from Environment import StochasticWindyGridworld\n",
    "from MonteCarlo import monte_carlo\n",
    "from Nstep import n_step_Q\n",
    "from Helper import LearningCurvePlot, smooth\n",
    "\n",
    "def average_over_repetitions(backup, n_repetitions, n_timesteps, max_episode_length, learning_rate, gamma, policy='egreedy', \n",
    "                    epsilon=None, temp=None, smoothing_window=51, plot=False, n=5):\n",
    "\n",
    "    reward_results = np.empty([n_repetitions,n_timesteps]) # Result array\n",
    "    now = time.time()\n",
    "    \n",
    "    for rep in range(n_repetitions): # Loop over repetitions\n",
    "        if backup == 'q':\n",
    "            rewards = q_learning(n_timesteps, learning_rate, gamma, policy, epsilon, temp, plot)\n",
    "        elif backup == 'sarsa':\n",
    "            rewards = sarsa(n_timesteps, learning_rate, gamma, policy, epsilon, temp, plot)\n",
    "        elif backup == 'mc':\n",
    "            rewards = monte_carlo(n_timesteps, max_episode_length, learning_rate, gamma, \n",
    "                   policy, epsilon, temp, plot)\n",
    "        elif backup == 'nstep':\n",
    "            rewards = n_step_Q(n_timesteps, max_episode_length, learning_rate, gamma, \n",
    "                   policy, epsilon, temp, plot, n=n)\n",
    "\n",
    "        reward_results[rep] = rewards\n",
    "        \n",
    "    print('Running one setting takes {} minutes'.format((time.time()-now)/60))    \n",
    "    learning_curve = np.mean(reward_results,axis=0) # average over repetitions\n",
    "    learning_curve = smooth(learning_curve,smoothing_window) # additional smoothing\n",
    "    return learning_curve  \n",
    "\n",
    "def experiment():\n",
    "    ####### Settings\n",
    "    # Experiment    \n",
    "    n_repetitions = 50\n",
    "    smoothing_window = 1001\n",
    "\n",
    "    # MDP    \n",
    "    n_timesteps = 50000\n",
    "    max_episode_length = 100\n",
    "    gamma = 1.0\n",
    "\n",
    "    # Exploration\n",
    "    policy = 'softmax' # 'egreedy' or 'softmax' \n",
    "    epsilon = 0.05\n",
    "    temp = 1.0\n",
    "    \n",
    "    # Target and update\n",
    "    backup = 'q' # 'q' or 'sarsa' or 'mc' or 'nstep'\n",
    "    learning_rate = 0.25\n",
    "    n = 5\n",
    "        \n",
    "    # Plotting parameters\n",
    "    plot = False\n",
    "    \n",
    "    # Nice labels for plotting\n",
    "    policy_labels = {'egreedy': '$\\epsilon$-greedy policy',\n",
    "                  'softmax': 'Softmax policy'}\n",
    "\n",
    "    backup_labels = {'q': 'Q-learning',\n",
    "                  'sarsa': 'SARSA',\n",
    "                  'mc': 'Monte Carlo',\n",
    "                  'nstep': 'n-step Q-learning'}\n",
    "    \n",
    "    ####### Experiments\n",
    "    \n",
    "    #### Assignment 1: Dynamic Programming\n",
    "    # Execute this assignment in DynamicProgramming.py\n",
    "    optimal_average_reward_per_timestep = 1.11 # set the optimal average reward per timestep you found in the DP assignment here\n",
    "   \n",
    "    # ##### Assignment 4: Back-up depth\n",
    "    backup = 'nstep'\n",
    "    ns = [1,3,5,10,20,100]\n",
    "    Plot = LearningCurvePlot(title = 'Effect of target depth')    \n",
    "    for n in ns:\n",
    "        learning_curve = average_over_repetitions(backup, n_repetitions, n_timesteps, max_episode_length, learning_rate, \n",
    "                                              gamma, policy, epsilon, temp, smoothing_window, plot, n)\n",
    "        Plot.add_curve(learning_curve,label=r'{}-step Q-learning'.format(n))\n",
    "    backup = 'mc'\n",
    "    learning_curve = average_over_repetitions(backup, n_repetitions, n_timesteps, max_episode_length, learning_rate, \n",
    "                                          gamma, policy, epsilon, temp, smoothing_window, plot, n)\n",
    "    Plot.add_curve(learning_curve,label='Monte Carlo')        \n",
    "    Plot.add_hline(optimal_average_reward_per_timestep, label=\"DP optimum\")\n",
    "    Plot.save('depth.png')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44860eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
