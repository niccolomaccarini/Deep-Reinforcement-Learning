{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e20e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.8.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from cartpole_environment import CartPoleEnv\n",
    "\n",
    "###SETUP###\n",
    "gamma = 0.995\n",
    "num_actions = 2\n",
    "state_shape = 4\n",
    "policy = 'egreedy'\n",
    "experience_replay = True\n",
    "decay_epsilon = True #This is to perform dynamic epsilon-greedy exploration by decaying the value of epsilon \n",
    "#with time\n",
    "\n",
    "epsilon = 1  # Epsilon greedy parameter\n",
    "epsilon_min = 0.01\n",
    "decay_rate = 0.9995\n",
    "batch_size = 20  # Size of batch taken from replay buffer\n",
    "\n",
    "game = CartPoleEnv()\n",
    "\n",
    "###BUILD THE ARCHITECTURE OF THE MODEL###\n",
    "def build_architecture(learning_rate = 0.001):\n",
    "    inputs = keras.Input(shape=(4,))\n",
    "    x = layers.Dense(24, activation = 'relu')(inputs)   #Tried with 100 nodes also, but apparently there's no improvement\n",
    "    x = layers.Dense(24, activation = 'relu')(x)\n",
    "    x = layers.Dense(24, activation = 'relu')(x) #Let's see what happens when removing a layer\n",
    "    outputs = layers.Dense(2, activation = 'linear')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer = optimizer, loss = 'mse')\n",
    "    return model\n",
    "\n",
    "def select_action(state, policy, epsilon, model):\n",
    "    if policy == 'egreedy':\n",
    "            if epsilon > np.random.rand(1)[0]:\n",
    "                action = random.randrange(game.action_space.n)\n",
    "            else:\n",
    "                # Predict action Q-values from environment state\n",
    "                action_probs = model.predict(np.array([state,]))\n",
    "                # Take best action\n",
    "                action = np.argmax(action_probs)\n",
    "    return action\n",
    "\n",
    "def experience_replay_update(batch_size, len_history, state_history,state_next_history,\n",
    "                             rewards_history, action_history, done_history, model):\n",
    "    # Get indices of samples for replay buffers\n",
    "    indices = np.random.choice(range(len_history), size = batch_size)\n",
    "\n",
    "    # Using list comprehension to sample from replay buffer\n",
    "    state_sample = np.array([state_history[i] for i in indices])\n",
    "    state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "    rewards_sample = np.array([rewards_history[i] for i in indices])\n",
    "    action_sample = [action_history[i] for i in indices]\n",
    "    done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "\n",
    "    # Build the updated Q-values for the sampled future states\n",
    "    # Q value = reward + discount factor * expected future reward\n",
    "            \n",
    "    y_train = model.predict(state_sample)\n",
    "    for i in range(len(y_train)):\n",
    "        if not done_sample[i]:\n",
    "            y_train[i][action_sample[i]] = rewards_sample[i] + gamma*np.max(model.predict(np.array([state_next_sample[i],])))\n",
    "        else:\n",
    "            y_train[i][action_sample[i]] = rewards_sample[i]\n",
    "    #Train the model\n",
    "    model.fit(state_sample, y_train, verbose = 0)\n",
    "\n",
    "def cartpole(n_runs, learning_rate, gamma, policy, epsilon, experience_replay, batch_size, decay_epsilon):\n",
    "    \n",
    "    model = build_architecture(learning_rate)\n",
    "    \n",
    "    ###Experience replay buffers###\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    state_next_history = []\n",
    "    rewards_history = []\n",
    "    done_history = []\n",
    "    episode_reward_history = []\n",
    "    running_reward = 0\n",
    "    # Maximum replay length\n",
    "    max_memory_length = 1000000\n",
    "    # Train the model after a fixed number of actions\n",
    "    run = 0\n",
    "\n",
    "    for i in range(n_runs):  # Run until we end the budget\n",
    "        state = game.reset()\n",
    "        #state = np.reshape(state, [1,state_shape])\n",
    "        #state = np.array([state,])\n",
    "        episode_reward = 0\n",
    "        run += 1\n",
    "        n_steps = 0\n",
    "    \n",
    "        while True:\n",
    "            #game.render() #Adding this line would show the attempts of the agent in a pop up window.\n",
    "            n_steps +=1\n",
    "            #Select an action according to the policy\n",
    "            action = select_action(state, policy, epsilon, model)\n",
    "            \n",
    "            # Decay the probability of taking random action\n",
    "            if decay_epsilon:\n",
    "                epsilon *= decay_rate\n",
    "                epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state_next, reward, done, _ = game.step(action)\n",
    "            #state_next = np.reshape(state_next, [1,state_shape])\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Save actions and states in replay buffer\n",
    "            action_history.append(action)\n",
    "            state_history.append(state)\n",
    "            state_next_history.append(state_next)\n",
    "            done_history.append(done)\n",
    "            rewards_history.append(reward)\n",
    "            state = state_next\n",
    "\n",
    "            # Update every fixed number of frames and once batch size is reached\n",
    "            if len(done_history) > batch_size and not done and experience_replay:\n",
    "                len_history = len(done_history)\n",
    "                experience_replay_update(batch_size, len_history, state_history, state_next_history, \n",
    "                                         rewards_history,\n",
    "                                         action_history, done_history, model)\n",
    "\n",
    "            # Limit the state and reward history\n",
    "            if len(rewards_history) > max_memory_length:\n",
    "                del rewards_history[:1]\n",
    "                del state_history[:1]\n",
    "                del state_next_history[:1]\n",
    "                del action_history[:1]\n",
    "                del done_history[:1]\n",
    "            \n",
    "            # If done print the score of current run\n",
    "            if done:\n",
    "                print(\"Run:\" + str(run) + \", Steps:\" + str(n_steps) + \", Epsilon:\" + str(epsilon))\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        episode_reward_history.append(episode_reward)\n",
    "\n",
    "    return episode_reward_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73617e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:1, Steps:34, Epsilon:0.17482461472379698\n",
      "Run:2, Steps:18, Epsilon:0.0694428401872336\n",
      "Run:3, Steps:10, Epsilon:0.041577993585724116\n",
      "Run:4, Steps:10, Epsilon:0.0248942806191894\n"
     ]
    }
   ],
   "source": [
    "cartpole(n_runs=2000, learning_rate=0.01, gamma=0.995, policy = 'egreedy', epsilon=1, experience_replay=True, batch_size=20, decay_epsilon=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
