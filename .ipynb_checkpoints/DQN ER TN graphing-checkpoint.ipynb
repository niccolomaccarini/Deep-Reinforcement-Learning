{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cfa50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:585: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object,\n",
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:637: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool,\n",
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:176: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object: SlowAppendObjectArrayToTensorProto,\n",
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:177: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool: SlowAppendBoolArrayToTensorProto,\n",
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_random.py:110: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def randint(low, high=None, size=None, dtype=onp.int):  # pylint: disable=missing-function-docstring\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from random import sample, randint\n",
    "from tensorflow import keras\n",
    "from Helper import argmax, softmax\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f431f906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def get_model(ip_shape,lr,op_shape,summary = True):\n",
    "    '''\n",
    "    get_model(ip_shape,lr,op_shape,summary = True):\n",
    "    creates and returns a model and prints it's summary based on summary flag\n",
    "    '''\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=ip_shape))\n",
    "    model.add(keras.layers.Dense(24, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(24, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(24, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(op_shape, activation=\"linear\"))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"mean_squared_error\",optimizer=keras.optimizers.Adam(learning_rate=lr),metrics=[\"accuracy\"])\n",
    "    if summary == True:\n",
    "        print(model.summary())\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d311caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_deque:\n",
    "    '''\n",
    "    __init__(self, max_len):\n",
    "    \n",
    "    # methods:\n",
    "    add_experience(self,s,a,r,s_next,done):\n",
    "    get_batch(self, batch_size):\n",
    "    '''\n",
    "    def __init__(self, max_len):\n",
    "        '''initialisation'''\n",
    "        #initialise max buffer length\n",
    "        self.deque_size = max_len\n",
    "        \n",
    "        # initialise buffer for live deque length\n",
    "        self.live_ds = 0\n",
    "        \n",
    "        # initialise experience buffers\n",
    "        self.s_experience = deque(maxlen = self.deque_size)\n",
    "        self.s_next_experience = deque(maxlen = self.deque_size)\n",
    "        self.a_experience = deque(maxlen = self.deque_size)\n",
    "        self.r_experience = deque(maxlen = self.deque_size)\n",
    "        self.d_experience = deque(maxlen = self.deque_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def add_experience(self,s,a,r,s_next,done):\n",
    "        '''\n",
    "        add_experience(self,s,a,r,s_next,done)\n",
    "        add an experience to the deques\n",
    "        '''\n",
    "        self.s_experience.append(s)\n",
    "        self.s_next_experience.append(s_next)\n",
    "        self.a_experience.append(a)\n",
    "        self.r_experience.append(r)\n",
    "        self.d_experience.append(done)\n",
    "        \n",
    "        #update live deque size\n",
    "        self.live_ds = len(self.s_experience)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def get_batch(self, batch_size):\n",
    "        '''\n",
    "        get_batch(self, batch_size):\n",
    "        generate random samples from experiences\n",
    "        returns them\n",
    "        '''\n",
    "        # warn that deque is not full\n",
    "        if self.live_ds < self.deque_size:\n",
    "            if self.live_ds%1000 == 0:\n",
    "                print(self.live_ds)\n",
    "                \n",
    "#             print(\"deque is not full, current size is : \", self.live_ds)\n",
    "#             if batch_size > self.live_ds:\n",
    "#                 print(\"batch size bigger than live deque size (bs,lds): \", batch_size, self.live_ds)\n",
    "#             else:\n",
    "#                 print(\"sampling from incomplete deque (bs,lds): \", batch_size, self.live_ds)\n",
    "        \n",
    "        # get random indices\n",
    "        ind = sample(range(self.live_ds), batch_size)\n",
    "        \n",
    "        # sample from all deques\n",
    "        s_sampled = np.asarray(self.s_experience)[ind]\n",
    "        s_next_sampled = np.asarray(self.s_next_experience)[ind]\n",
    "        a_sampled = np.asarray(self.a_experience)[ind]\n",
    "        r_sampled = np.asarray(self.r_experience)[ind]\n",
    "        d_sampled = np.asarray(self.d_experience)[ind]\n",
    "        \n",
    "        return (s_sampled,s_next_sampled,a_sampled,r_sampled,d_sampled)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50c2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNagent:\n",
    "    '''\n",
    "    __init__(self, n_states, n_actions, learning_rate, gamma, max_len, batch_size,\n",
    "    er = True, tn = True, conv = False, summary = True, verbose = 0):\n",
    "    Sets up a model and provides handy methods to interact with it\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, env, buffer, live_model, target_model,  gamma, batch_size, er = True,\n",
    "                 tn = True, summary = True, verbose = 2):\n",
    "        '''Iniitialization function for class DQNagent, read the __docs__'''\n",
    "        \n",
    "        # used for \n",
    "        self.buffer = buffer\n",
    "        self.live_model = live_model\n",
    "        self.target_model = target_model\n",
    "        self.n_states = env.observation_space.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "#         self.deque_size = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def select_action(self, state, policy='egreedy', epsilon=None, temp=None):\n",
    "        '''\n",
    "        select_action(self, s, policy='egreedy', epsilon=None, temp=None):\n",
    "        selects action based on policy specified\n",
    "        returns action\n",
    "        '''\n",
    "        state = state.reshape(1,4)\n",
    "        if policy == 'egreedy':\n",
    "            if epsilon is None:\n",
    "                raise KeyError(\"Provide an epsilon\")\n",
    "                \n",
    "            # TO DO: Add own code\n",
    "            exploit = np.random.choice([0,1],p = [epsilon,1-epsilon])\n",
    "            if exploit:\n",
    "#                 print(q[0])\n",
    "                a = argmax((self.live_model.predict(state))[0])\n",
    "#                 print(\"exploiting: \",a)\n",
    "            else:\n",
    "                a = np.random.randint(0,self.n_actions) # Replace this with correct action selection\n",
    "#                 print(\"exploring: \",a)\n",
    "                \n",
    "#         elif policy == 'softmax':\n",
    "#             if temp is None:\n",
    "#                 raise KeyError(\"Provide a temperature\")\n",
    "                \n",
    "#             # TO DO: Add own code\n",
    "#             a = np.random.randint(0,self.n_actions) # Replace this with correct action selection\n",
    "#             print(\"action selected :\", a)\n",
    "        return a\n",
    "                \n",
    "\n",
    "    '''\n",
    "    def update(self,s,a,r,s_next,done):\n",
    "        #perform a Q-learning update\n",
    "\n",
    "        # TO DO: Add own code\n",
    "        G = r + ( self.gamma * np.max(self.Q_sa[s_next,:]) )\n",
    "        self.Q_sa[s,a] = self.Q_sa[s,a] + ( self.learning_rate * ( G - self.Q_sa[s,a] ) )\n",
    "        pass\n",
    "    \n",
    "#                     def fit_batch(env, model, target_model, batch):\n",
    "#                        observations, actions, rewards, next_observations, dones = batch\n",
    "#                        # Predict the Q values of the next states. Passing ones as the action mask.\n",
    "#                        next_q_values = predict(env, target_model, next_observations)\n",
    "#                        # The Q values of terminal states is 0 by definition.\n",
    "#                        next_q_values[dones] = 0.0\n",
    "#                        # The Q values of each start state is the reward + gamma * the max next state Q value\n",
    "#                        q_values = rewards + DISCOUNT_FACTOR_GAMMA * np.max(next_q_values, axis=1)\n",
    "#                        one_hot_actions = np.array([one_hot_encode(env.action_space.n, action) for action in actions])\n",
    "#                        history = model.fit(\n",
    "#                            x=[observations, one_hot_actions],\n",
    "#                            y=one_hot_actions * q_values[:, None],\n",
    "#                            batch_size=BATCH_SIZE,\n",
    "#                            verbose=0,\n",
    "#                        )\n",
    "#                        return history.history['loss'][0]\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def update_er_tn(self,batch_of_ss, batch_of_as, batch_of_sns, batch_of_rs,batch_of_ds):\n",
    "        '''\n",
    "        update_er_tn(self,batch):\n",
    "        perform a Q-learning update\n",
    "        '''\n",
    "\n",
    "        # get target Q values for the batch\n",
    "        q_next_batch = self.target_model.predict(batch_of_sns)\n",
    "        \n",
    "        # calculate targets and assign done rewards\n",
    "        G1 = self.live_model.predict(batch_of_ss)\n",
    "        cat_boa = to_categorical(batch_of_as,num_classes = 2)\n",
    "        cat_inv_boa = to_categorical(np.invert(batch_of_as),num_classes = 2)\n",
    "        G1 = G1 * cat_inv_boa\n",
    "        G2 = batch_of_rs + ( self.gamma * np.max(q_next_batch, axis = 1) )\n",
    "        G2 = np.where(batch_of_ds == True, batch_of_rs, G2)\n",
    "        G2 = G2.reshape(G2.shape[0],1)\n",
    "        G2 = G2 * cat_boa\n",
    "        G_batch = G1+G2\n",
    "\n",
    "        \n",
    "        # update live_network\n",
    "        history = self.live_model.fit(batch_of_ss,G_batch, batch_size = self.batch_size, verbose = 0)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def synch_weights(self):\n",
    "        '''\n",
    "        synch_weights(self):\n",
    "        synchronises target model weights\n",
    "        '''\n",
    "        self.target_model.set_weights(self.live_model.get_weights())\n",
    "        print(\"weights synched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a32aafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_eps(epsilon = 1, min_epsilon = 0.01, decay_rate = 0.95):\n",
    "    '''\n",
    "    decay_eps(max_epsilon = 1, min_epsilon = 0.01, decay_rate = 0.95):\n",
    "    decay the epsilon value\n",
    "    '''\n",
    "    epsilon *= decay_rate\n",
    "    return max(epsilon, min_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43db2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "def Qlearn(learning_rate, epsilon, buffer_size, n_eps, max_timesteps,batch_size, min_buff = 100, decay_epsilon = True):\n",
    "    \n",
    "    # initialise environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    # create buffers for kpi\n",
    "    reward_per_step_per_ep = [] # list of cum_reward at each step [n_eps, n_timesteps]\n",
    "    cum_reward_per_ep = [] # list of final rewards [n_eps]\n",
    "\n",
    "    # initialise networks\n",
    "    live_net = get_model(env.observation_space.shape,learning_rate,env.action_space.n)\n",
    "    target_net = get_model(env.observation_space.shape,learning_rate,env.action_space.n)\n",
    "    \n",
    "    # initialise buffers\n",
    "    buffer = experience_deque(max_len = buffer_size)\n",
    "    \n",
    "    # initialise agent\n",
    "#         def __init__(self, env, buffer, live_model, target_model,  gamma, batch_size, er = True,\n",
    "#                  tn = True, summary = True, verbose = 2):\n",
    "    agent = DQNagent(env, buffer, live_net, target_net, gamma, batch_size)\n",
    "    \n",
    "    # count for target net update\n",
    "    step_count = 0\n",
    "    \n",
    "    # loop over eps\n",
    "    for ep_num in range(n_eps):\n",
    "        s = env.reset()\n",
    "        rewards = []\n",
    "        cum_reward = 0\n",
    "        # loop over timesteps\n",
    "        for step in range(max_timesteps):\n",
    "            step_count += 1\n",
    "#             env.render()\n",
    "            \n",
    "            # select action\n",
    "            a = agent.select_action(state = s,policy = 'egreedy', epsilon = epsilon)\n",
    "#             print(a)\n",
    "#             print(epsilon)\n",
    "\n",
    "            \n",
    "            # play a step\n",
    "            s_next,reward,done,_ = env.step(a)\n",
    "            cum_reward += reward\n",
    "            \n",
    "            # save experience\n",
    "            buffer.add_experience(s,a,reward,s_next,done)\n",
    "        \n",
    "            if buffer.live_ds >= min_buff:\n",
    "                batch_size = min(int(0.7*buffer.live_ds),500)\n",
    "                \n",
    "                # run a DQN training loop\n",
    "                s_exp_batch,s_next_exp_batch,a_exp_batch,r_exp_batch,d_exp_batch = buffer.get_batch(batch_size)\n",
    "                history = agent.update_er_tn(s_exp_batch,a_exp_batch,s_next_exp_batch,r_exp_batch,d_exp_batch)\n",
    "#                 print(\"loss = \",history.history[\"loss\"][0])\n",
    "\n",
    "                # copy weights to target_network\n",
    "                if step_count%synch_weights_freq == 0:\n",
    "                    agent.synch_weights()\n",
    "    #             print(\"trainstep success\")\n",
    "    \n",
    "#             else:\n",
    "#                 print(\"only adding to buffer\")\n",
    "\n",
    "            \n",
    "            # create reward per step\n",
    "            rewards.append(cum_reward)\n",
    "            \n",
    "            # check if done\n",
    "            if done:\n",
    "#                 print(\"done = \",done)\n",
    "                print(\"ep = \", ep_num, \"    steps = \", step,\"ep reward = \",cum_reward)\n",
    "                break\n",
    "                \n",
    "        \n",
    "            # set new state\n",
    "            s = s_next\n",
    "        \n",
    "        #\n",
    "        reward_per_step_per_ep.append(rewards)\n",
    "        cum_reward_per_ep.append(cum_reward)\n",
    "        # Decay probability of taking random action\n",
    "        if decay_epsilon:\n",
    "            epsilon = decay_eps(epsilon,eps_min)\n",
    "                \n",
    "    return (reward_per_step_per_ep,cum_reward_per_ep)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad0e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eps = 1000\n",
    "buffer_size = 10000\n",
    "synch_weights_freq = 20\n",
    "learning_rate = 0.001\n",
    "gamma = 0.999\n",
    "epsilon = 1\n",
    "batch_size = 500\n",
    "min_buff = 1000\n",
    "max_timesteps = 600\n",
    "eps_min = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c70fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 1,370\n",
      "Trainable params: 1,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 1,370\n",
      "Trainable params: 1,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "ep =  0     steps =  13 ep reward =  14.0\n",
      "ep =  1     steps =  36 ep reward =  37.0\n",
      "ep =  2     steps =  32 ep reward =  33.0\n",
      "ep =  3     steps =  29 ep reward =  30.0\n",
      "ep =  4     steps =  34 ep reward =  35.0\n",
      "ep =  5     steps =  84 ep reward =  85.0\n",
      "ep =  6     steps =  17 ep reward =  18.0\n",
      "ep =  7     steps =  29 ep reward =  30.0\n",
      "ep =  8     steps =  22 ep reward =  23.0\n",
      "ep =  9     steps =  13 ep reward =  14.0\n",
      "ep =  10     steps =  33 ep reward =  34.0\n",
      "ep =  11     steps =  126 ep reward =  127.0\n",
      "ep =  12     steps =  76 ep reward =  77.0\n",
      "ep =  13     steps =  18 ep reward =  19.0\n",
      "ep =  14     steps =  49 ep reward =  50.0\n",
      "ep =  15     steps =  32 ep reward =  33.0\n",
      "ep =  16     steps =  71 ep reward =  72.0\n",
      "ep =  17     steps =  142 ep reward =  143.0\n",
      "ep =  18     steps =  99 ep reward =  100.0\n",
      "1000\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  19     steps =  75 ep reward =  76.0\n",
      "ep =  20     steps =  8 ep reward =  9.0\n",
      "weights synched\n",
      "ep =  21     steps =  11 ep reward =  12.0\n",
      "weights synched\n",
      "ep =  22     steps =  17 ep reward =  18.0\n",
      "weights synched\n",
      "ep =  23     steps =  17 ep reward =  18.0\n",
      "ep =  24     steps =  9 ep reward =  10.0\n",
      "weights synched\n",
      "ep =  25     steps =  8 ep reward =  9.0\n",
      "weights synched\n",
      "ep =  26     steps =  17 ep reward =  18.0\n",
      "weights synched\n",
      "ep =  27     steps =  18 ep reward =  19.0\n",
      "ep =  28     steps =  9 ep reward =  10.0\n",
      "weights synched\n",
      "ep =  29     steps =  9 ep reward =  10.0\n",
      "ep =  30     steps =  8 ep reward =  9.0\n",
      "weights synched\n",
      "ep =  31     steps =  7 ep reward =  8.0\n",
      "ep =  32     steps =  11 ep reward =  12.0\n",
      "weights synched\n",
      "ep =  33     steps =  8 ep reward =  9.0\n",
      "ep =  34     steps =  7 ep reward =  8.0\n",
      "ep =  35     steps =  9 ep reward =  10.0\n",
      "weights synched\n",
      "ep =  36     steps =  9 ep reward =  10.0\n",
      "weights synched\n",
      "ep =  37     steps =  16 ep reward =  17.0\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  38     steps =  38 ep reward =  39.0\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  39     steps =  51 ep reward =  52.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  40     steps =  70 ep reward =  71.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  41     steps =  101 ep reward =  102.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  42     steps =  175 ep reward =  176.0\n",
      "weights synched\n"
     ]
    }
   ],
   "source": [
    "reward_per_step_per_ep,cum_reward_per_e = Qlearn(learning_rate, epsilon, buffer_size, n_eps, max_timesteps, batch_size, min_buff = min_buff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81091fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
