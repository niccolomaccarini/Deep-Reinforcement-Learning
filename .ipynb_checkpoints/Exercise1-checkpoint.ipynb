{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4268d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 3, Action 1, Reward -1.0, Next state 10, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 10, Action 3, Reward -1.0, Next state 3, p(s'|s,a) [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 3, Action 2, Reward -1.0, Next state 2, p(s'|s,a) [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 2, Action 3, Reward -1.0, Next state 2, p(s'|s,a) [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 2, Action 1, Reward -1.0, Next state 9, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 9, Action 2, Reward -1.0, Next state 8, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 8, Action 1, Reward -1.0, Next state 15, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 15, Action 3, Reward -1.0, Next state 8, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 8, Action 1, Reward -1.0, Next state 15, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 15, Action 0, Reward -1.0, Next state 16, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 16, Action 2, Reward -1.0, Next state 15, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 15, Action 2, Reward -1.0, Next state 14, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 14, Action 2, Reward -1.0, Next state 14, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 14, Action 0, Reward -1.0, Next state 15, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 15, Action 1, Reward -1.0, Next state 23, p(s'|s,a) [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.2 0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 23, Action 3, Reward -1.0, Next state 16, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 16, Action 2, Reward -1.0, Next state 15, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 15, Action 1, Reward -1.0, Next state 22, p(s'|s,a) [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.2 0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 22, Action 0, Reward -1.0, Next state 24, p(s'|s,a) [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.2 0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 24, Action 2, Reward -1.0, Next state 24, p(s'|s,a) [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.2 0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 24, Action 2, Reward -1.0, Next state 24, p(s'|s,a) [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.2 0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 24, Action 0, Reward -1.0, Next state 25, p(s'|s,a) [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.2 0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 25, Action 0, Reward -1.0, Next state 27, p(s'|s,a) [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.8 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 27, Action 3, Reward -1.0, Next state 20, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "State 20, Action 1, Reward -1.0, Next state 27, p(s'|s,a) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], r(s,a,s') [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Practical for course 'Reinforcement Learning',\n",
    "Leiden University, The Netherlands\n",
    "2022\n",
    "By Thomas Moerland\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "#%matplotlib ipympl\n",
    "matplotlib.use('Qt5Agg') # or TkAgg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle,Circle,Arrow\n",
    "\n",
    "class StochasticWindyGridworld:\n",
    "    ''' Stochastic version of WindyGridworld \n",
    "        (Sutton & Barto, Example 6.5 at page 130, see http://incompleteideas.net/book/RLbook2020.pdf)\n",
    "        Compared to the book version, the vertical wind is now stochastic, and only blows 80% of the times\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,initialize_model=True):\n",
    "        self.height = 7\n",
    "        self.width = 10\n",
    "        self.reward_per_step = -1.0\n",
    "        self.goal_reward = 35\n",
    "        self.shape = (self.width, self.height)\n",
    "        self.n_states = self.height * self.width\n",
    "        self.n_actions = 4\n",
    "        self.winds = (0,0,0,1,1,1,2,2,1,0)\n",
    "        self.wind_blows_proportion = 0.8         \n",
    "        self.action_effects = {\n",
    "                0: (0, 1),  # up\n",
    "                1: (1, 0),   # right\n",
    "                2: (0, -1),   # down\n",
    "                3: (-1, 0),  # left\n",
    "                }\n",
    "        self.initialize_model = initialize_model\n",
    "        if initialize_model:\n",
    "            self._construct_model()\n",
    "        self.fig = None\n",
    "        self.Q_labels = None\n",
    "        self.arrows = None\n",
    "        self.reset() # set agent to the start location\n",
    "\n",
    "    def reset(self):\n",
    "        ''' set the agent back to the start location '''\n",
    "        self.agent_location = np.array([0,3])\n",
    "        s = self._location_to_state(self.agent_location)\n",
    "        return s\n",
    "    \n",
    "    def step(self,a):\n",
    "        ''' Forward the environment based on action a, really affecting the agent location  \n",
    "        Returns the next state, the obtained reward, and a boolean whether the environment terminated '''\n",
    "        self.agent_location += self.action_effects[a] # effect of action\n",
    "        self.agent_location = np.clip(self.agent_location,(0,0),np.array(self.shape)-1) # bound within grid\n",
    "        if np.random.rand() < self.wind_blows_proportion: # apply effect of wind\n",
    "            self.agent_location[1] += self.winds[self.agent_location[0]] # effect of wind\n",
    "        self.agent_location = np.clip(self.agent_location,(0,0),np.array(self.shape)-1) # bound within grid\n",
    "        s_next = self._location_to_state(self.agent_location)    \n",
    "        \n",
    "        # Check reward and termination\n",
    "        if np.all(self.agent_location == (7,3)):\n",
    "            done = True\n",
    "            r = self.goal_reward\n",
    "        else:\n",
    "            done = False\n",
    "            r = self.reward_per_step\n",
    "            \n",
    "        return s_next, r, done  \n",
    "\n",
    "    def model(self,s,a):\n",
    "        ''' Returns vectors p(s'|s,a) and r(s,a,s') for given s and a.\n",
    "        Only simulates, does not affect the current agent location '''\n",
    "        if self.initialize_model:\n",
    "            return self.p_sas[s,a], self.r_sas[s,a]\n",
    "        else:\n",
    "            raise ValueError(\"set initialize_model=True when creating Environment\")\n",
    "            \n",
    "\n",
    "    def render(self,Q_sa=None,plot_optimal_policy=False,step_pause=0.001):\n",
    "        ''' Plot the environment \n",
    "        if Q_sa is provided, it will also plot the Q(s,a) values for each action in each state\n",
    "        if plot_optimal_policy=True, it will additionally add an arrow in each state to indicate the greedy action '''\n",
    "        # Initialize figure\n",
    "        if self.fig == None:\n",
    "            self._initialize_plot()\n",
    "            \n",
    "        # Add Q-values to plot\n",
    "        if Q_sa is not None:\n",
    "            # Initialize labels\n",
    "            if self.Q_labels is None:\n",
    "                self._initialize_Q_labels()\n",
    "            # Set correct values of labels\n",
    "            for state in range(self.n_states):\n",
    "                for action in range(self.n_actions):\n",
    "                    self.Q_labels[state][action].set_text(np.round(Q_sa[state,action],1))\n",
    "\n",
    "        # Add arrows of optimal policy\n",
    "        if plot_optimal_policy and Q_sa is not None:\n",
    "            self._plot_arrows(Q_sa)\n",
    "            \n",
    "        # Update agent location\n",
    "        self.agent_circle.center = self.agent_location+0.5\n",
    "            \n",
    "        # Draw figure\n",
    "        plt.pause(step_pause)    \n",
    "\n",
    "    def _state_to_location(self,state):\n",
    "        ''' bring a state index to an (x,y) location of the agent '''\n",
    "        return np.array(np.unravel_index(state,self.shape))\n",
    "    \n",
    "    def _location_to_state(self,location):\n",
    "        ''' bring an (x,y) location of the agent to a state index '''\n",
    "        return np.ravel_multi_index(location,self.shape)\n",
    "        \n",
    "    def _construct_model(self):\n",
    "        ''' Constructs full p(s'|s,a) and r(s,a,s') arrays\n",
    "            Stores these in self.p_sas and self.r_sas '''\n",
    "            \n",
    "        # Initialize transition and reward functions\n",
    "        p_sas = np.zeros((self.n_states,self.n_actions,self.n_states))\n",
    "        r_sas = np.zeros((self.n_states,self.n_actions,self.n_states)) + self.reward_per_step \n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                s_location = self._state_to_location(s)  \n",
    "                    \n",
    "                # if s is terminal state make it a self-loop without rewards\n",
    "                if np.all(s_location == (7,3)):\n",
    "                    goal_state = self._location_to_state((7,3))\n",
    "                    p_sas[s,a,goal_state] = 1.0 \n",
    "                    r_sas[s,a,] = np.zeros(self.n_states)  \n",
    "                    continue\n",
    "\n",
    "                # check what happens if the wind blows:\n",
    "                next_location_with_wind = np.copy(s_location) \n",
    "                next_location_with_wind += self.action_effects[a] # effect of action\n",
    "                next_location_with_wind = np.clip(next_location_with_wind,(0,0),np.array(self.shape)-1) # bound within grid\n",
    "                next_location_with_wind[1] += self.winds[next_location_with_wind[0]] # Apply effect of wind\n",
    "                next_location_with_wind = np.clip(next_location_with_wind,(0,0),np.array(self.shape)-1) # bound within grid\n",
    "                next_state_with_wind = self._location_to_state(next_location_with_wind)   \n",
    "                \n",
    "                # Update p_sas and r_sas\n",
    "                p_sas[s,a,next_state_with_wind] += self.wind_blows_proportion\n",
    "                if np.all(next_location_with_wind == (7,3)):\n",
    "                    r_sas[s,a,next_state_with_wind]  = self.goal_reward\n",
    "        \n",
    "                # check what happens if the wind blows:\n",
    "                next_location_without_wind = np.copy(s_location)\n",
    "                next_location_without_wind += self.action_effects[a] # effect of action\n",
    "                next_location_without_wind = np.clip(next_location_without_wind,(0,0),np.array(self.shape)-1) # bound within grid\n",
    "                next_state_without_wind = self._location_to_state(next_location_without_wind)\n",
    "\n",
    "                # Update p_sas and r_sas\n",
    "                p_sas[s,a,next_state_without_wind] += (1-self.wind_blows_proportion)\n",
    "                if np.all(next_location_without_wind == (7,3)):\n",
    "                    r_sas[s,a,next_state_without_wind]  = self.goal_reward  \n",
    "\n",
    "        self.p_sas = p_sas\n",
    "        self.r_sas = r_sas\n",
    "        return \n",
    "\n",
    "    def _initialize_plot(self):\n",
    "        self.fig,self.ax = plt.subplots()#figsize=(self.width, self.height+1)) # Start a new figure\n",
    "        self.ax.set_xlim([0,self.width])\n",
    "        self.ax.set_ylim([0,self.height]) \n",
    "        self.ax.axes.xaxis.set_visible(False)\n",
    "        self.ax.axes.yaxis.set_visible(False)\n",
    "\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                self.ax.add_patch(Rectangle((x, y),1,1, linewidth=0, facecolor='k',alpha=self.winds[x]/4))       \n",
    "                self.ax.add_patch(Rectangle((x, y),1,1, linewidth=0.5, edgecolor='k', fill=False))     \n",
    "\n",
    "        self.ax.axvline(0,0,self.height,linewidth=5,c='k')\n",
    "        self.ax.axvline(self.width,0,self.height,linewidth=5,c='k')\n",
    "        self.ax.axhline(0,0,self.width,linewidth=5,c='k')\n",
    "        self.ax.axhline(self.height,0,self.width,linewidth=5,c='k')\n",
    "\n",
    "\n",
    "        # Indicate start and goal state\n",
    "        self.ax.add_patch(Rectangle((0.0, 3.0),1.0,1,0, linewidth=0, facecolor='r',alpha=0.2))\n",
    "        self.ax.add_patch(Rectangle((7.0, 3.0),1.0,1,0, linewidth=0, facecolor='g',alpha=0.2))\n",
    "        self.ax.text(0.05,3.75, 'S', fontsize=20, c='r')\n",
    "        self.ax.text(7.05,3.75, 'G', fontsize=20, c='g')\n",
    "\n",
    "\n",
    "        # Add agent\n",
    "        self.agent_circle = Circle(self.agent_location+0.5,0.3)\n",
    "        self.ax.add_patch(self.agent_circle)\n",
    "        \n",
    "    def _initialize_Q_labels(self):\n",
    "        self.Q_labels = []\n",
    "        for state in range(self.n_states):\n",
    "            state_location = self._state_to_location(state)\n",
    "            self.Q_labels.append([])\n",
    "            for action in range(self.n_actions):\n",
    "                plot_location = np.array(state_location) + 0.42 + 0.35 * np.array(self.action_effects[action])\n",
    "                next_label = self.ax.text(plot_location[0],plot_location[1]+0.03,0.0,fontsize=8)\n",
    "                self.Q_labels[state].append(next_label)\n",
    "\n",
    "    def _plot_arrows(self,Q_sa):\n",
    "        if self.arrows is not None: \n",
    "            for arrow in self.arrows:\n",
    "                arrow.remove() # Clear all previous arrows\n",
    "        self.arrows=[]\n",
    "        for state in range(self.n_states):\n",
    "            plot_location = np.array(self._state_to_location(state)) + 0.5\n",
    "            max_actions = full_argmax(Q_sa[state])\n",
    "            for max_action in max_actions:\n",
    "                new_arrow = arrow = Arrow(plot_location[0],plot_location[1],self.action_effects[max_action][0]*0.2,\n",
    "                                          self.action_effects[max_action][1]*0.2, width=0.05,color='k')\n",
    "                ax_arrow = self.ax.add_patch(new_arrow)\n",
    "                self.arrows.append(ax_arrow)\n",
    "\n",
    "def full_argmax(x):\n",
    "    ''' Own variant of np.argmax, since np.argmax only returns the first occurence of the max '''\n",
    "    return np.where(x == np.max(x))[0]            \n",
    "\n",
    "def test():\n",
    "    # Hyperparameters\n",
    "    n_test_steps = 25\n",
    "    step_pause = 0.5\n",
    "    \n",
    "    # Initialize environment and Q-array\n",
    "    env = StochasticWindyGridworld()\n",
    "    s = env.reset()\n",
    "    Q_sa = np.zeros((env.n_states,env.n_actions)) # Q-value array of flat zeros\n",
    "\n",
    "    # Test\n",
    "    for t in range(n_test_steps):\n",
    "        a = np.random.randint(4) # sample random action    \n",
    "        s_next,r,done = env.step(a) # execute action in the environment\n",
    "        p_sas,r_sas = env.model(s,a)\n",
    "        print(\"State {}, Action {}, Reward {}, Next state {}, p(s'|s,a) {}, r(s,a,s') {}\".format(s,a,r,s_next,p_sas,r_sas))\n",
    "        env.render(Q_sa=Q_sa,plot_optimal_policy=False,step_pause=step_pause) # display the environment\n",
    "        s = s_next\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbba7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
