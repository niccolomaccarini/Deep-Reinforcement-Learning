{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ff8f68e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-8676dac3a113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-8676dac3a113>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mplot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Obtained rewards: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-8676dac3a113>\u001b[0m in \u001b[0;36mq_learning\u001b[1;34m(n_timesteps, learning_rate, gamma, policy, epsilon, temp, plot)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms_next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_sa\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_sa\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplot_optimal_policy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep_pause\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.000001\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Plot the Q-value estimates during Q-learning execution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\RL Assignemnt1\\Environment.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, Q_sa, plot_optimal_policy, step_pause)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;31m# Add arrows of optimal policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mplot_optimal_policy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mQ_sa\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_arrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_sa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m# Update agent location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\RL Assignemnt1\\Environment.py\u001b[0m in \u001b[0;36m_plot_arrows\u001b[1;34m(self, Q_sa)\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mmax_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_argmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_sa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmax_action\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmax_actions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                 new_arrow = arrow = Arrow(plot_location[0],plot_location[1],self.action_effects[max_action][0]*0.2,\n\u001b[0m\u001b[0;32m    212\u001b[0m                                           self.action_effects[max_action][1]*0.2, width=0.05,color='k')\n\u001b[0;32m    213\u001b[0m                 \u001b[0max_arrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_arrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, dx, dy, width, **kwargs)\u001b[0m\n\u001b[0;32m   1227\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m         self._patch_transform = (\n\u001b[1;32m-> 1229\u001b[1;33m             \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAffine2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1230\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marctan2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mrotate\u001b[1;34m(self, theta)\u001b[0m\n\u001b[0;32m   1931\u001b[0m         rotate_mtx = np.array([[a, -b, 0.0], [b, a, 0.0], [0.0, 0.0, 1.0]],\n\u001b[0;32m   1932\u001b[0m                               float)\n\u001b[1;32m-> 1933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotate_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1934\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1935\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Practical for course 'Reinforcement Learning',\n",
    "Leiden University, The Netherlands\n",
    "2021\n",
    "By Thomas Moerland\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg') # or TkAgg\n",
    "import numpy as np\n",
    "from Environment import StochasticWindyGridworld\n",
    "from Helper import softmax, argmax\n",
    "\n",
    "class QLearningAgent:\n",
    "\n",
    "    def __init__(self, n_states, n_actions, learning_rate, gamma):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.Q_sa = np.zeros((n_states,n_actions))\n",
    "        \n",
    "    def select_action(self, s, policy='egreedy', epsilon=None, temp=None):\n",
    "        \n",
    "        actions = list(range(self.n_actions))\n",
    "        \n",
    "        if policy == 'egreedy':\n",
    "            if epsilon is None:\n",
    "                raise KeyError(\"Provide an epsilon\")\n",
    "            b = argmax([self.Q_sa[s,b] for b in range(self.n_actions)])\n",
    "            actions.append(b)\n",
    "            a = np.random.choice(actions,p=[epsilon/self.n_actions for i in range(self.n_actions)] + [1 - epsilon])\n",
    "                \n",
    "        elif policy == 'softmax':\n",
    "            if temp is None:\n",
    "                raise KeyError(\"Provide a temperature\")\n",
    "            probabilities = softmax(np.array(self.Q_sa[s,:]), temp)\n",
    "            a = np.random.choice(actions, p=probabilities)\n",
    "            \n",
    "        return a\n",
    "        \n",
    "    def update(self,s,a,r,s_next,done):\n",
    "        G = r + self.gamma*np.max([self.Q_sa[s_next, b] for b in range(self.n_actions)])\n",
    "        if not done:\n",
    "            self.Q_sa[s,a] += self.learning_rate*(G - self.Q_sa[s,a])\n",
    "        else:\n",
    "            self.Q_sa[s,a] += self.learning_rate*(r - self.Q_sa[s,a])\n",
    "        \n",
    "\n",
    "def q_learning(n_timesteps, learning_rate, gamma, policy='egreedy', epsilon=None, temp=None, plot=True):\n",
    "    ''' runs a single repetition of q_learning\n",
    "    Return: rewards, a vector with the observed rewards at each timestep ''' \n",
    "    \n",
    "    env = StochasticWindyGridworld(initialize_model=False)\n",
    "    pi = QLearningAgent(env.n_states, env.n_actions, learning_rate, gamma)\n",
    "    rewards = []\n",
    "\n",
    "    s = env.reset()\n",
    "    budget = n_timesteps\n",
    "    \n",
    "    while budget:\n",
    "        budget = budget - 1\n",
    "        a = pi.select_action(s, policy = policy, epsilon = epsilon, temp = temp)\n",
    "        s_next, r, done  = env.step(a)\n",
    "        pi.update(s, a, r, s_next, done)\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            s = s_next\n",
    "        if plot:\n",
    "            env.render(Q_sa=pi.Q_sa,plot_optimal_policy=True,step_pause=0.000001) # Plot the Q-value estimates during Q-learning execution\n",
    "        \n",
    "    return rewards \n",
    "\n",
    "def test():\n",
    "    \n",
    "    n_timesteps = 50000\n",
    "    gamma = 1.0\n",
    "    learning_rate = 0.25\n",
    "\n",
    "    # Exploration\n",
    "    policy = 'softmax' # 'egreedy' or 'softmax' \n",
    "    epsilon = 0.01\n",
    "    temp = 0.1\n",
    "    \n",
    "    # Plotting parameters\n",
    "    plot = True\n",
    "\n",
    "    rewards = q_learning(n_timesteps, learning_rate, gamma, policy, epsilon, temp, plot)\n",
    "    print(\"Obtained rewards: {}\".format(rewards))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006e0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
