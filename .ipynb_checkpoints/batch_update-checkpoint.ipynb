{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e20e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from cartpole_environment import CartPoleEnv\n",
    "\n",
    "###SETUP###\n",
    "gamma = 0.995\n",
    "num_actions = 2\n",
    "state_shape = 4\n",
    "policy = 'egreedy'\n",
    "experience_replay = True\n",
    "decay_epsilon = True #This is to perform dynamic epsilon-greedy exploration by decaying the value of epsilon \n",
    "#with time\n",
    "\n",
    "epsilon = 1  # Epsilon greedy parameter\n",
    "epsilon_min = 0.001\n",
    "decay_rate = 0.95\n",
    "batch_size = 64  # Size of batch taken from replay buffer\n",
    "\n",
    "game = CartPoleEnv()\n",
    "\n",
    "###BUILD THE ARCHITECTURE OF THE MODEL###\n",
    "def build_architecture(learning_rate = 0.001):\n",
    "    inputs = keras.Input(shape=(4,))\n",
    "    x = layers.Dense(24, activation = 'relu')(inputs)   #Tried with 100 nodes also, but apparently there's no improvement\n",
    "    x = layers.Dense(24, activation = 'relu')(x)\n",
    "    #x = layers.Dense(24, activation = 'relu')(x) #Let's see what happens when removing a layer\n",
    "    outputs = layers.Dense(2, activation = 'linear')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer = optimizer, loss = 'mse')\n",
    "    return model\n",
    "\n",
    "def select_action(state, policy, epsilon, model):\n",
    "    if policy == 'egreedy':\n",
    "            if epsilon > np.random.rand(1)[0]:\n",
    "                action = random.randrange(game.action_space.n)\n",
    "            else:\n",
    "                # Predict action Q-values from environment state\n",
    "                action_probs = model.predict(np.array([state,]))\n",
    "                # Take best action\n",
    "                action = np.argmax(action_probs)\n",
    "    return action\n",
    "\n",
    "def experience_replay_update(batch_size, len_history, state_history,state_next_history,\n",
    "                             rewards_history, action_history, done_history, model):\n",
    "    # Get indices of samples for replay buffers\n",
    "    indices = np.random.choice(range(len_history), size = batch_size)\n",
    "\n",
    "    # Using list comprehension to sample from replay buffer\n",
    "    state_sample = np.array([state_history[i] for i in indices])\n",
    "    state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "    rewards_sample = np.array([rewards_history[i] for i in indices])\n",
    "    action_sample = [action_history[i] for i in indices]\n",
    "    done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "\n",
    "    # Build the updated Q-values for the sampled future states\n",
    "    # Q value = reward + discount factor * expected future reward\n",
    "            \n",
    "    y_train = model.predict(state_sample)\n",
    "    for i in range(len(y_train)):\n",
    "        if not done_sample[i]:\n",
    "            y_train[i][action_sample[i]] = rewards_sample[i] + gamma*np.max(model.predict(np.array([state_next_sample[i],])))\n",
    "        else:\n",
    "            y_train[i][action_sample[i]] = rewards_sample[i]\n",
    "    #Train the model\n",
    "    model.fit(state_sample, y_train, verbose = 0)\n",
    "\n",
    "def cartpole(n_runs, learning_rate, gamma, policy, epsilon, experience_replay, batch_size, decay_epsilon):\n",
    "    \n",
    "    model = build_architecture(learning_rate)\n",
    "    \n",
    "    ###Experience replay buffers###\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    state_next_history = []\n",
    "    rewards_history = []\n",
    "    done_history = []\n",
    "    episode_reward_history = []\n",
    "    running_reward = 0\n",
    "    # Maximum replay length\n",
    "    max_memory_length = 1000000\n",
    "    # Train the model after a fixed number of actions\n",
    "    run = 0\n",
    "\n",
    "    for i in range(n_runs):  # Run until we end the budget\n",
    "        state = game.reset()\n",
    "        #state = np.reshape(state, [1,state_shape])\n",
    "        #state = np.array([state,])\n",
    "        episode_reward = 0\n",
    "        run += 1\n",
    "        n_steps = 0\n",
    "    \n",
    "        while True:\n",
    "            #game.render() #Adding this line would show the attempts of the agent in a pop up window.\n",
    "            n_steps +=1\n",
    "            #Select an action according to the policy\n",
    "            action = select_action(state, policy, epsilon, model)\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state_next, reward, done, _ = game.step(action)\n",
    "            #state_next = np.reshape(state_next, [1,state_shape])\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Save actions and states in replay buffer\n",
    "            action_history.append(action)\n",
    "            state_history.append(state)\n",
    "            state_next_history.append(state_next)\n",
    "            done_history.append(done)\n",
    "            rewards_history.append(reward)\n",
    "            state = state_next\n",
    "\n",
    "            # Update every fixed number of frames and once batch size is reached\n",
    "            if len(done_history) > batch_size and not done and experience_replay:\n",
    "                len_history = len(done_history)\n",
    "                experience_replay_update(batch_size, len_history, state_history, state_next_history, \n",
    "                                         rewards_history,\n",
    "                                         action_history, done_history, model)\n",
    "\n",
    "            # Limit the state and reward history\n",
    "            if len(rewards_history) > max_memory_length:\n",
    "                del rewards_history[:1]\n",
    "                del state_history[:1]\n",
    "                del state_next_history[:1]\n",
    "                del action_history[:1]\n",
    "                del done_history[:1]\n",
    "            \n",
    "            # If done print the score of current run\n",
    "            if done:\n",
    "                print(\"Run:\" + str(run) + \", Steps:\" + str(n_steps) + \", Epsilon:\" + str(epsilon))\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        episode_reward_history.append(episode_reward)\n",
    "        \n",
    "        # Decay the probability of taking random action\n",
    "        if decay_epsilon:\n",
    "            epsilon *= decay_rate\n",
    "            epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "    return episode_reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa217802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:1, Steps:11, Epsilon:1\n",
      "Run:2, Steps:32, Epsilon:0.95\n",
      "Run:3, Steps:16, Epsilon:0.9025\n",
      "Run:4, Steps:12, Epsilon:0.8573749999999999\n",
      "Run:5, Steps:10, Epsilon:0.8145062499999999\n",
      "Run:6, Steps:16, Epsilon:0.7737809374999999\n",
      "Run:7, Steps:27, Epsilon:0.7350918906249998\n",
      "Run:8, Steps:13, Epsilon:0.6983372960937497\n",
      "Run:9, Steps:18, Epsilon:0.6634204312890623\n",
      "Run:10, Steps:11, Epsilon:0.6302494097246091\n",
      "Run:11, Steps:25, Epsilon:0.5987369392383786\n",
      "Run:12, Steps:12, Epsilon:0.5688000922764596\n",
      "Run:13, Steps:8, Epsilon:0.5403600876626365\n",
      "Run:14, Steps:10, Epsilon:0.5133420832795047\n",
      "Run:15, Steps:17, Epsilon:0.48767497911552943\n",
      "Run:16, Steps:13, Epsilon:0.46329123015975293\n",
      "Run:17, Steps:54, Epsilon:0.44012666865176525\n",
      "Run:18, Steps:22, Epsilon:0.41812033521917696\n",
      "Run:19, Steps:39, Epsilon:0.3972143184582181\n",
      "Run:20, Steps:22, Epsilon:0.37735360253530714\n",
      "Run:21, Steps:26, Epsilon:0.35848592240854177\n",
      "Run:22, Steps:20, Epsilon:0.34056162628811465\n",
      "Run:23, Steps:28, Epsilon:0.3235335449737089\n",
      "Run:24, Steps:18, Epsilon:0.30735686772502346\n",
      "Run:25, Steps:28, Epsilon:0.2919890243387723\n",
      "Run:26, Steps:16, Epsilon:0.27738957312183365\n",
      "Run:27, Steps:33, Epsilon:0.263520094465742\n",
      "Run:28, Steps:34, Epsilon:0.25034408974245487\n",
      "Run:29, Steps:85, Epsilon:0.2378268852553321\n",
      "Run:30, Steps:15, Epsilon:0.2259355409925655\n",
      "Run:31, Steps:25, Epsilon:0.2146387639429372\n",
      "Run:32, Steps:25, Epsilon:0.20390682574579033\n",
      "Run:33, Steps:55, Epsilon:0.1937114844585008\n",
      "Run:34, Steps:43, Epsilon:0.18402591023557577\n",
      "Run:35, Steps:38, Epsilon:0.17482461472379698\n",
      "Run:36, Steps:26, Epsilon:0.16608338398760714\n",
      "Run:37, Steps:38, Epsilon:0.15777921478822676\n",
      "Run:38, Steps:31, Epsilon:0.14989025404881542\n",
      "Run:39, Steps:50, Epsilon:0.14239574134637464\n",
      "Run:40, Steps:37, Epsilon:0.1352759542790559\n",
      "Run:41, Steps:32, Epsilon:0.1285121565651031\n",
      "Run:42, Steps:42, Epsilon:0.12208654873684793\n",
      "Run:43, Steps:31, Epsilon:0.11598222130000553\n",
      "Run:44, Steps:45, Epsilon:0.11018311023500525\n",
      "Run:45, Steps:29, Epsilon:0.10467395472325498\n",
      "Run:46, Steps:48, Epsilon:0.09944025698709223\n",
      "Run:47, Steps:75, Epsilon:0.09446824413773762\n",
      "Run:48, Steps:112, Epsilon:0.08974483193085074\n",
      "Run:49, Steps:102, Epsilon:0.0852575903343082\n",
      "Run:50, Steps:141, Epsilon:0.08099471081759278\n",
      "Run:51, Steps:134, Epsilon:0.07694497527671314\n",
      "Run:52, Steps:51, Epsilon:0.07309772651287748\n"
     ]
    }
   ],
   "source": [
    "cartpole(n_runs=2000, learning_rate=0.01, gamma=0.995, policy = 'egreedy', epsilon=1, experience_replay=True, batch_size=64, decay_epsilon=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
