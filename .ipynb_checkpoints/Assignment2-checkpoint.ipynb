{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015fa1dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.8.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from cartpole_environment import CartPoleEnv #'cartpole_environment' is how I named the gym file, possibly needs to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7caf919a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "###SETUP###\n",
    "gamma = 0.995\n",
    "num_actions = 2\n",
    "state_shape = 4\n",
    "policy = 'egreedy'\n",
    "experience_replay = True\n",
    "decay_epsilon = True #This is to perform dynamic epsilon-greedy exploration by decaying the value of epsilon \n",
    "#with time\n",
    "\n",
    "epsilon = 1  # Epsilon greedy parameter\n",
    "epsilon_min = 0.01\n",
    "decay_rate = 0.9995\n",
    "batch_size = 20  # Size of batch taken from replay buffer\n",
    "\n",
    "game = CartPoleEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6c5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BUILD THE ARCHITECTURE OF THE MODEL###\n",
    "def build_architecture(learning_rate = 0.001):\n",
    "    inputs = keras.Input(shape=(4,))\n",
    "    x = layers.Dense(24, activation = 'relu')(inputs)   #Tried with 100 nodes also, but apparently there's no improvement\n",
    "    x = layers.Dense(24, activation = 'relu')(x)\n",
    "    x = layers.Dense(24, activation = 'relu')(x) #Let's see what happens when removing a layer\n",
    "    outputs = layers.Dense(2, activation = 'linear')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer = optimizer, loss = 'mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09fcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy, epsilon, decay_epsilon, model):\n",
    "    if policy == 'egreedy':\n",
    "            if epsilon > np.random.rand(1)[0]:\n",
    "                action = random.randrange(game.action_space.n)\n",
    "            else:\n",
    "                # Predict action Q-values from environment state\n",
    "                action_probs = model.predict(state)\n",
    "                # Take best action\n",
    "                action = np.argmax(action_probs)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "695fe14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay_update(batch_size, len_history, state_history, state_next_history, rewards_history, action_history, done_history,model):\n",
    "    # Get indices of samples for replay buffers\n",
    "    indices = np.random.choice(range(len_history), size = batch_size)\n",
    "\n",
    "    # Using list comprehension to sample from replay buffer\n",
    "    state_sample = np.array([state_history[i] for i in indices])\n",
    "    state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "    rewards_sample = np.array([rewards_history[i] for i in indices])\n",
    "    action_sample = [action_history[i] for i in indices]\n",
    "    done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "\n",
    "    # Build the updated Q-values for the sampled future states\n",
    "    # Q value = reward + discount factor * expected future reward\n",
    "            \n",
    "    for i in range(batch_size):\n",
    "        y_train = model.predict(state_sample[i])\n",
    "        if not done_sample[i]:\n",
    "            y_train[0][action_sample[i]] = rewards_sample[i] + gamma*np.max(model.predict(state_next_sample[i]))\n",
    "        else:\n",
    "            y_train[0][action_sample[i]] = rewards_sample[i]\n",
    "        #Train the model\n",
    "        model.fit(state_sample[i], y_train, verbose = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06100b6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cartpole(n_runs, learning_rate, gamma, policy, epsilon, experience_replay, batch_size, decay_epsilon):\n",
    "    \n",
    "    model = build_architecture(learning_rate)\n",
    "    \n",
    "    ###Experience replay buffers###\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    state_next_history = []\n",
    "    rewards_history = []\n",
    "    done_history = []\n",
    "    episode_reward_history = []\n",
    "    running_reward = 0\n",
    "    # Maximum replay length\n",
    "    max_memory_length = 1000000\n",
    "    # Train the model after a fixed number of actions\n",
    "    run = 0\n",
    "\n",
    "    for i in range(n_runs):  # Run until solved\n",
    "        state = game.reset()\n",
    "        state = np.reshape(state, [1,state_shape])\n",
    "        episode_reward = 0\n",
    "        run += 1\n",
    "        n_steps = 0\n",
    "    \n",
    "        while True:\n",
    "            #game.render() #Adding this line would show the attempts of the agent in a pop up window.\n",
    "            n_steps +=1\n",
    "            #Select an action according to the policy\n",
    "            action = select_action(state, policy, epsilon, decay_epsilon, model)\n",
    "            \n",
    "            # Decay probability of taking random action\n",
    "            if decay_epsilon:\n",
    "                epsilon *= decay_rate\n",
    "                epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state_next, reward, done, _ = game.step(action)\n",
    "            state_next = np.reshape(state_next, [1,state_shape])\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Save actions and states in replay buffer\n",
    "            action_history.append(action)\n",
    "            state_history.append(state)\n",
    "            state_next_history.append(state_next)\n",
    "            done_history.append(done)\n",
    "            rewards_history.append(reward)\n",
    "            state = state_next\n",
    "\n",
    "            # Update every fixed number of frames and once batch size is reached\n",
    "            if len(done_history) > batch_size and not done and experience_replay:\n",
    "                len_history = len(done_history)\n",
    "                experience_replay_update(batch_size, len_history, state_history, state_next_history, \n",
    "                                         rewards_history, action_history, done_history,model)\n",
    "\n",
    "            # Limit the state and reward history\n",
    "            if len(rewards_history) > max_memory_length:\n",
    "                del rewards_history[:1]\n",
    "                del state_history[:1]\n",
    "                del state_next_history[:1]\n",
    "                del action_history[:1]\n",
    "                del done_history[:1]\n",
    "            \n",
    "            # If done print the score of current run\n",
    "            if done:\n",
    "                print(\"Run:\" + str(run) + \", Steps:\" + str(n_steps) + \", Epsilon:\" + str(epsilon))\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        episode_reward_history.append(episode_reward)\n",
    "\n",
    "    return episode_reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8013d6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:1, Steps:19, Epsilon:0.990542629116888\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e2bd618aaed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcartpole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_runs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.995\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'egreedy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience_replay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_epsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-4f961fe8661b>\u001b[0m in \u001b[0;36mcartpole\u001b[1;34m(n_runs, learning_rate, gamma, policy, epsilon, experience_replay, batch_size, decay_epsilon)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone_history\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mexperience_replay\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mlen_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 experience_replay_update(batch_size, len_history, state_history, state_next_history, \n\u001b[0m\u001b[0;32m     54\u001b[0m                                          rewards_history, action_history, done_history)\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e9e81b7761d3>\u001b[0m in \u001b[0;36mexperience_replay_update\u001b[1;34m(batch_size, len_history, state_history, state_next_history, rewards_history, action_history, done_history)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_next_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "cartpole(n_runs=2000, learning_rate=0.01, gamma=0.995, policy = 'egreedy', epsilon=1, experience_replay=True, batch_size=20, decay_epsilon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064984c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
