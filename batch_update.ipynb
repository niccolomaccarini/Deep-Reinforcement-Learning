{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e20e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from cartpole_environment import CartPoleEnv\n",
    "\n",
    "###SETUP###\n",
    "gamma = 0.995\n",
    "num_actions = 2\n",
    "state_shape = 4\n",
    "policy = 'egreedy'\n",
    "experience_replay = True\n",
    "decay_epsilon = True #This is to perform dynamic epsilon-greedy exploration by decaying the value of epsilon \n",
    "#with time\n",
    "\n",
    "epsilon = 1  # Epsilon greedy parameter\n",
    "epsilon_min = 0.001\n",
    "decay_rate = 0.95\n",
    "batch_size = 64  # Size of batch taken from replay buffer\n",
    "\n",
    "game = CartPoleEnv()\n",
    "\n",
    "###BUILD THE ARCHITECTURE OF THE MODEL###\n",
    "def build_architecture(learning_rate = 0.001):\n",
    "    inputs = keras.Input(shape=(4,))\n",
    "    x = layers.Dense(24, activation = 'relu')(inputs)   #Tried with 100 nodes also, but apparently there's no improvement\n",
    "    x = layers.Dense(24, activation = 'relu')(x)\n",
    "    #x = layers.Dense(24, activation = 'relu')(x) #Let's see what happens when removing a layer\n",
    "    outputs = layers.Dense(2, activation = 'linear')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer = optimizer, loss = 'mse')\n",
    "    return model\n",
    "\n",
    "def select_action(state, policy, epsilon, model):\n",
    "    if policy == 'egreedy':\n",
    "            if epsilon > np.random.rand(1)[0]:\n",
    "                action = random.randrange(game.action_space.n)\n",
    "            else:\n",
    "                # Predict action Q-values from environment state\n",
    "                action_probs = model.predict(np.array([state,]))\n",
    "                # Take best action\n",
    "                action = np.argmax(action_probs)\n",
    "    return action\n",
    "\n",
    "def experience_replay_update(batch_size, len_history, state_history,state_next_history,\n",
    "                             rewards_history, action_history, done_history, model):\n",
    "    # Get indices of samples for replay buffers\n",
    "    indices = np.random.choice(range(len_history), size = batch_size)\n",
    "\n",
    "    # Using list comprehension to sample from replay buffer\n",
    "    state_sample = np.array([state_history[i] for i in indices])\n",
    "    state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "    rewards_sample = np.array([rewards_history[i] for i in indices])\n",
    "    action_sample = [action_history[i] for i in indices]\n",
    "    done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "\n",
    "    # Build the updated Q-values for the sampled future states\n",
    "    # Q value = reward + discount factor * expected future reward\n",
    "            \n",
    "    y_train = model.predict(state_sample)\n",
    "    for i in range(len(y_train)):\n",
    "        if not done_sample[i]:\n",
    "            y_train[i][action_sample[i]] = rewards_sample[i] + gamma*np.max(model.predict(np.array([state_next_sample[i],])))\n",
    "        else:\n",
    "            y_train[i][action_sample[i]] = rewards_sample[i]\n",
    "    #Train the model\n",
    "    model.fit(state_sample, y_train, verbose = 0)\n",
    "\n",
    "def cartpole(n_runs, learning_rate, gamma, policy, epsilon, experience_replay, batch_size, decay_epsilon):\n",
    "    \n",
    "    model = build_architecture(learning_rate)\n",
    "    \n",
    "    ###Experience replay buffers###\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    state_next_history = []\n",
    "    rewards_history = []\n",
    "    done_history = []\n",
    "    episode_reward_history = []\n",
    "    running_reward = 0\n",
    "    # Maximum replay length\n",
    "    max_memory_length = 1000000\n",
    "    # Train the model after a fixed number of actions\n",
    "    run = 0\n",
    "\n",
    "    for i in range(n_runs):  # Run until we end the budget\n",
    "        state = game.reset()\n",
    "        #state = np.reshape(state, [1,state_shape])\n",
    "        #state = np.array([state,])\n",
    "        episode_reward = 0\n",
    "        run += 1\n",
    "        n_steps = 0\n",
    "    \n",
    "        while True:\n",
    "            #game.render() #Adding this line would show the attempts of the agent in a pop up window.\n",
    "            n_steps +=1\n",
    "            #Select an action according to the policy\n",
    "            action = select_action(state, policy, epsilon, model)\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state_next, reward, done, _ = game.step(action)\n",
    "            #state_next = np.reshape(state_next, [1,state_shape])\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Save actions and states in replay buffer\n",
    "            action_history.append(action)\n",
    "            state_history.append(state)\n",
    "            state_next_history.append(state_next)\n",
    "            done_history.append(done)\n",
    "            rewards_history.append(reward)\n",
    "            state = state_next\n",
    "\n",
    "            # Update every fixed number of frames and once batch size is reached\n",
    "            if len(done_history) > batch_size and not done and experience_replay:\n",
    "                len_history = len(done_history)\n",
    "                experience_replay_update(batch_size, len_history, state_history, state_next_history, \n",
    "                                         rewards_history,\n",
    "                                         action_history, done_history, model)\n",
    "\n",
    "            # Limit the state and reward history\n",
    "            if len(rewards_history) > max_memory_length:\n",
    "                del rewards_history[:1]\n",
    "                del state_history[:1]\n",
    "                del state_next_history[:1]\n",
    "                del action_history[:1]\n",
    "                del done_history[:1]\n",
    "            \n",
    "            # If done print the score of current run\n",
    "            if done:\n",
    "                print(\"Run:\" + str(run) + \", Steps:\" + str(n_steps) + \", Epsilon:\" + str(epsilon))\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        episode_reward_history.append(episode_reward)\n",
    "        \n",
    "        # Decay the probability of taking random action\n",
    "        if decay_epsilon:\n",
    "            epsilon *= decay_rate\n",
    "            epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "    return episode_reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa217802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:1, Steps:11, Epsilon:1\n",
      "Run:2, Steps:32, Epsilon:0.95\n",
      "Run:3, Steps:16, Epsilon:0.9025\n",
      "Run:4, Steps:12, Epsilon:0.8573749999999999\n",
      "Run:5, Steps:10, Epsilon:0.8145062499999999\n",
      "Run:6, Steps:16, Epsilon:0.7737809374999999\n",
      "Run:7, Steps:27, Epsilon:0.7350918906249998\n",
      "Run:8, Steps:13, Epsilon:0.6983372960937497\n",
      "Run:9, Steps:18, Epsilon:0.6634204312890623\n",
      "Run:10, Steps:11, Epsilon:0.6302494097246091\n",
      "Run:11, Steps:25, Epsilon:0.5987369392383786\n",
      "Run:12, Steps:12, Epsilon:0.5688000922764596\n",
      "Run:13, Steps:8, Epsilon:0.5403600876626365\n",
      "Run:14, Steps:10, Epsilon:0.5133420832795047\n",
      "Run:15, Steps:17, Epsilon:0.48767497911552943\n",
      "Run:16, Steps:13, Epsilon:0.46329123015975293\n",
      "Run:17, Steps:54, Epsilon:0.44012666865176525\n",
      "Run:18, Steps:22, Epsilon:0.41812033521917696\n",
      "Run:19, Steps:39, Epsilon:0.3972143184582181\n",
      "Run:20, Steps:22, Epsilon:0.37735360253530714\n",
      "Run:21, Steps:26, Epsilon:0.35848592240854177\n",
      "Run:22, Steps:20, Epsilon:0.34056162628811465\n",
      "Run:23, Steps:28, Epsilon:0.3235335449737089\n",
      "Run:24, Steps:18, Epsilon:0.30735686772502346\n",
      "Run:25, Steps:28, Epsilon:0.2919890243387723\n",
      "Run:26, Steps:16, Epsilon:0.27738957312183365\n",
      "Run:27, Steps:33, Epsilon:0.263520094465742\n",
      "Run:28, Steps:34, Epsilon:0.25034408974245487\n",
      "Run:29, Steps:85, Epsilon:0.2378268852553321\n",
      "Run:30, Steps:15, Epsilon:0.2259355409925655\n",
      "Run:31, Steps:25, Epsilon:0.2146387639429372\n",
      "Run:32, Steps:25, Epsilon:0.20390682574579033\n",
      "Run:33, Steps:55, Epsilon:0.1937114844585008\n",
      "Run:34, Steps:43, Epsilon:0.18402591023557577\n",
      "Run:35, Steps:38, Epsilon:0.17482461472379698\n",
      "Run:36, Steps:26, Epsilon:0.16608338398760714\n",
      "Run:37, Steps:38, Epsilon:0.15777921478822676\n",
      "Run:38, Steps:31, Epsilon:0.14989025404881542\n",
      "Run:39, Steps:50, Epsilon:0.14239574134637464\n",
      "Run:40, Steps:37, Epsilon:0.1352759542790559\n",
      "Run:41, Steps:32, Epsilon:0.1285121565651031\n",
      "Run:42, Steps:42, Epsilon:0.12208654873684793\n",
      "Run:43, Steps:31, Epsilon:0.11598222130000553\n",
      "Run:44, Steps:45, Epsilon:0.11018311023500525\n",
      "Run:45, Steps:29, Epsilon:0.10467395472325498\n",
      "Run:46, Steps:48, Epsilon:0.09944025698709223\n",
      "Run:47, Steps:75, Epsilon:0.09446824413773762\n",
      "Run:48, Steps:112, Epsilon:0.08974483193085074\n",
      "Run:49, Steps:102, Epsilon:0.0852575903343082\n",
      "Run:50, Steps:141, Epsilon:0.08099471081759278\n",
      "Run:51, Steps:134, Epsilon:0.07694497527671314\n",
      "Run:52, Steps:51, Epsilon:0.07309772651287748\n",
      "Run:53, Steps:286, Epsilon:0.0694428401872336\n",
      "Run:54, Steps:312, Epsilon:0.0659706981778719\n",
      "Run:55, Steps:261, Epsilon:0.0626721632689783\n",
      "Run:56, Steps:484, Epsilon:0.059538555105529384\n",
      "Run:57, Steps:184, Epsilon:0.05656162735025291\n",
      "Run:58, Steps:267, Epsilon:0.053733545982740265\n",
      "Run:59, Steps:247, Epsilon:0.05104686868360325\n",
      "Run:60, Steps:335, Epsilon:0.04849452524942309\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a0d420c7c639>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcartpole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_runs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.995\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'egreedy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience_replay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_epsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-0ac76ff618eb>\u001b[0m in \u001b[0;36mcartpole\u001b[1;34m(n_runs, learning_rate, gamma, policy, epsilon, experience_replay, batch_size, decay_epsilon)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone_history\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mexperience_replay\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[0mlen_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                 experience_replay_update(batch_size, len_history, state_history, state_next_history, \n\u001b[0m\u001b[0;32m    121\u001b[0m                                          \u001b[0mrewards_history\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                                          action_history, done_history, model)\n",
      "\u001b[1;32m<ipython-input-7-0ac76ff618eb>\u001b[0m in \u001b[0;36mexperience_replay_update\u001b[1;34m(batch_size, len_history, state_history, state_next_history, rewards_history, action_history, done_history, model)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_next_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1774\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal_pss_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1776\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1778\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    510\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \"\"\"\n\u001b[0;32m   1093\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1094\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1095\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cartpole(n_runs=2000, learning_rate=0.01, gamma=0.995, policy = 'egreedy', epsilon=1, experience_replay=True, batch_size=64, decay_epsilon=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
