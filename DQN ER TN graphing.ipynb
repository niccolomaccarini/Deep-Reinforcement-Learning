{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cfa50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:585: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object,\n",
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:637: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool,\n",
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:176: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object: SlowAppendObjectArrayToTensorProto,\n",
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:177: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool: SlowAppendBoolArrayToTensorProto,\n",
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_random.py:110: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def randint(low, high=None, size=None, dtype=onp.int):  # pylint: disable=missing-function-docstring\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from random import sample, randint\n",
    "from tensorflow import keras\n",
    "from Helper import argmax, softmax\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f431f906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def get_model(ip_shape,lr,op_shape,summary = True):\n",
    "    '''\n",
    "    get_model(ip_shape,lr,op_shape,summary = True):\n",
    "    creates and returns a model and prints it's summary based on summary flag\n",
    "    '''\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=ip_shape))\n",
    "    model.add(keras.layers.Dense(24, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(24, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(24, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(op_shape, activation=\"linear\"))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"mean_squared_error\",optimizer=keras.optimizers.Adam(learning_rate=lr),metrics=[\"accuracy\"])\n",
    "    if summary == True:\n",
    "        print(model.summary())\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d311caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_deque:\n",
    "    '''\n",
    "    __init__(self, max_len):\n",
    "    \n",
    "    # methods:\n",
    "    add_experience(self,s,a,r,s_next,done):\n",
    "    get_batch(self, batch_size):\n",
    "    '''\n",
    "    def __init__(self, max_len):\n",
    "        '''initialisation'''\n",
    "        #initialise max buffer length\n",
    "        self.deque_size = max_len\n",
    "        \n",
    "        # initialise buffer for live deque length\n",
    "        self.live_ds = 0\n",
    "        \n",
    "        # initialise experience buffers\n",
    "        self.s_experience = deque(maxlen = self.deque_size)\n",
    "        self.s_next_experience = deque(maxlen = self.deque_size)\n",
    "        self.a_experience = deque(maxlen = self.deque_size)\n",
    "        self.r_experience = deque(maxlen = self.deque_size)\n",
    "        self.d_experience = deque(maxlen = self.deque_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def add_experience(self,s,a,r,s_next,done):\n",
    "        '''\n",
    "        add_experience(self,s,a,r,s_next,done)\n",
    "        add an experience to the deques\n",
    "        '''\n",
    "        self.s_experience.append(s)\n",
    "        self.s_next_experience.append(s_next)\n",
    "        self.a_experience.append(a)\n",
    "        self.r_experience.append(r)\n",
    "        self.d_experience.append(done)\n",
    "        \n",
    "        #update live deque size\n",
    "        self.live_ds = len(self.s_experience)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def get_batch(self, batch_size):\n",
    "        '''\n",
    "        get_batch(self, batch_size):\n",
    "        generate random samples from experiences\n",
    "        returns them\n",
    "        '''\n",
    "        # warn that deque is not full\n",
    "        if self.live_ds < self.deque_size:\n",
    "            if self.live_ds%1000 == 0:\n",
    "                print(self.live_ds)\n",
    "                \n",
    "#             print(\"deque is not full, current size is : \", self.live_ds)\n",
    "#             if batch_size > self.live_ds:\n",
    "#                 print(\"batch size bigger than live deque size (bs,lds): \", batch_size, self.live_ds)\n",
    "#             else:\n",
    "#                 print(\"sampling from incomplete deque (bs,lds): \", batch_size, self.live_ds)\n",
    "        \n",
    "        # get random indices\n",
    "        ind = sample(range(self.live_ds), batch_size)\n",
    "        \n",
    "        # sample from all deques\n",
    "        s_sampled = np.asarray(self.s_experience)[ind]\n",
    "        s_next_sampled = np.asarray(self.s_next_experience)[ind]\n",
    "        a_sampled = np.asarray(self.a_experience)[ind]\n",
    "        r_sampled = np.asarray(self.r_experience)[ind]\n",
    "        d_sampled = np.asarray(self.d_experience)[ind]\n",
    "        \n",
    "        return (s_sampled,s_next_sampled,a_sampled,r_sampled,d_sampled)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50c2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNagent:\n",
    "    '''\n",
    "    __init__(self, n_states, n_actions, learning_rate, gamma, max_len, batch_size,\n",
    "    er = True, tn = True, conv = False, summary = True, verbose = 0):\n",
    "    Sets up a model and provides handy methods to interact with it\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, env, buffer, live_model, target_model,  gamma, batch_size, er = True,\n",
    "                 tn = True, summary = True, verbose = 2):\n",
    "        '''Iniitialization function for class DQNagent, read the __docs__'''\n",
    "        \n",
    "        # used for \n",
    "        self.buffer = buffer\n",
    "        self.live_model = live_model\n",
    "        self.target_model = target_model\n",
    "        self.n_states = env.observation_space.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "#         self.deque_size = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def select_action(self, state, policy='egreedy', epsilon=None, temp=None):\n",
    "        '''\n",
    "        select_action(self, s, policy='egreedy', epsilon=None, temp=None):\n",
    "        selects action based on policy specified\n",
    "        returns action\n",
    "        '''\n",
    "        state = state.reshape(1,4)\n",
    "        if policy == 'egreedy':\n",
    "            if epsilon is None:\n",
    "                raise KeyError(\"Provide an epsilon\")\n",
    "                \n",
    "            # TO DO: Add own code\n",
    "            exploit = np.random.choice([0,1],p = [epsilon,1-epsilon])\n",
    "            if exploit:\n",
    "#                 print(q[0])\n",
    "                a = argmax((self.live_model.predict(state))[0])\n",
    "#                 print(\"exploiting: \",a)\n",
    "            else:\n",
    "                a = np.random.randint(0,self.n_actions) # Replace this with correct action selection\n",
    "#                 print(\"exploring: \",a)\n",
    "                \n",
    "#         elif policy == 'softmax':\n",
    "#             if temp is None:\n",
    "#                 raise KeyError(\"Provide a temperature\")\n",
    "                \n",
    "#             # TO DO: Add own code\n",
    "#             a = np.random.randint(0,self.n_actions) # Replace this with correct action selection\n",
    "#             print(\"action selected :\", a)\n",
    "        return a\n",
    "                \n",
    "\n",
    "    '''\n",
    "    def update(self,s,a,r,s_next,done):\n",
    "        #perform a Q-learning update\n",
    "\n",
    "        # TO DO: Add own code\n",
    "        G = r + ( self.gamma * np.max(self.Q_sa[s_next,:]) )\n",
    "        self.Q_sa[s,a] = self.Q_sa[s,a] + ( self.learning_rate * ( G - self.Q_sa[s,a] ) )\n",
    "        pass\n",
    "    \n",
    "#                     def fit_batch(env, model, target_model, batch):\n",
    "#                        observations, actions, rewards, next_observations, dones = batch\n",
    "#                        # Predict the Q values of the next states. Passing ones as the action mask.\n",
    "#                        next_q_values = predict(env, target_model, next_observations)\n",
    "#                        # The Q values of terminal states is 0 by definition.\n",
    "#                        next_q_values[dones] = 0.0\n",
    "#                        # The Q values of each start state is the reward + gamma * the max next state Q value\n",
    "#                        q_values = rewards + DISCOUNT_FACTOR_GAMMA * np.max(next_q_values, axis=1)\n",
    "#                        one_hot_actions = np.array([one_hot_encode(env.action_space.n, action) for action in actions])\n",
    "#                        history = model.fit(\n",
    "#                            x=[observations, one_hot_actions],\n",
    "#                            y=one_hot_actions * q_values[:, None],\n",
    "#                            batch_size=BATCH_SIZE,\n",
    "#                            verbose=0,\n",
    "#                        )\n",
    "#                        return history.history['loss'][0]\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def update_er_tn(self,batch_of_ss, batch_of_as, batch_of_sns, batch_of_rs,batch_of_ds):\n",
    "        '''\n",
    "        update_er_tn(self,batch):\n",
    "        perform a Q-learning update\n",
    "        '''\n",
    "\n",
    "        # get target Q values for the batch\n",
    "        q_next_batch = self.target_model.predict(batch_of_sns)\n",
    "        \n",
    "        # calculate targets and assign done rewards\n",
    "        G1 = self.live_model.predict(batch_of_ss)\n",
    "        cat_boa = to_categorical(batch_of_as,num_classes = 2)\n",
    "        cat_inv_boa = to_categorical(np.invert(batch_of_as),num_classes = 2)\n",
    "        G1 = G1 * cat_inv_boa\n",
    "        G2 = batch_of_rs + ( self.gamma * np.max(q_next_batch, axis = 1) )\n",
    "        G2 = np.where(batch_of_ds == True, batch_of_rs, G2)\n",
    "        G2 = G2.reshape(G2.shape[0],1)\n",
    "        G2 = G2 * cat_boa\n",
    "        G_batch = G1+G2\n",
    "\n",
    "        \n",
    "        # update live_network\n",
    "        history = self.live_model.fit(batch_of_ss,G_batch, batch_size = self.batch_size, verbose = 0)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def synch_weights(self):\n",
    "        '''\n",
    "        synch_weights(self):\n",
    "        synchronises target model weights\n",
    "        '''\n",
    "        self.target_model.set_weights(self.live_model.get_weights())\n",
    "        print(\"weights synched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a32aafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_eps(epsilon = 1, min_epsilon = 0.01, decay_rate = 0.95):\n",
    "    '''\n",
    "    decay_eps(max_epsilon = 1, min_epsilon = 0.01, decay_rate = 0.95):\n",
    "    decay the epsilon value\n",
    "    '''\n",
    "    epsilon *= decay_rate\n",
    "    return max(epsilon, min_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43db2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "def Qlearn(learning_rate, epsilon, buffer_size, n_eps, max_timesteps,batch_size, min_buff = 100, decay_epsilon = True):\n",
    "    \n",
    "    # initialise environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    # create buffers for kpi\n",
    "    reward_per_step_per_ep = [] # list of cum_reward at each step [n_eps, n_timesteps]\n",
    "    cum_reward_per_ep = [] # list of final rewards [n_eps]\n",
    "\n",
    "    # initialise networks\n",
    "    live_net = get_model(env.observation_space.shape,learning_rate,env.action_space.n)\n",
    "    target_net = get_model(env.observation_space.shape,learning_rate,env.action_space.n)\n",
    "    \n",
    "    # initialise buffers\n",
    "    buffer = experience_deque(max_len = buffer_size)\n",
    "    \n",
    "    # initialise agent\n",
    "#         def __init__(self, env, buffer, live_model, target_model,  gamma, batch_size, er = True,\n",
    "#                  tn = True, summary = True, verbose = 2):\n",
    "    agent = DQNagent(env, buffer, live_net, target_net, gamma, batch_size)\n",
    "    \n",
    "    # count for target net update\n",
    "    step_count = 0\n",
    "    \n",
    "    # loop over eps\n",
    "    for ep_num in range(n_eps):\n",
    "        s = env.reset()\n",
    "        rewards = []\n",
    "        cum_reward = 0\n",
    "        # loop over timesteps\n",
    "        for step in range(max_timesteps):\n",
    "            step_count += 1\n",
    "#             env.render()\n",
    "            \n",
    "            # select action\n",
    "            a = agent.select_action(state = s,policy = 'egreedy', epsilon = epsilon)\n",
    "#             print(a)\n",
    "#             print(epsilon)\n",
    "\n",
    "            \n",
    "            # play a step\n",
    "            s_next,reward,done,_ = env.step(a)\n",
    "            cum_reward += reward\n",
    "            \n",
    "            # save experience\n",
    "            buffer.add_experience(s,a,reward,s_next,done)\n",
    "        \n",
    "            if buffer.live_ds >= min_buff:\n",
    "                batch_size = min(int(0.7*buffer.live_ds),500)\n",
    "                \n",
    "                # run a DQN training loop\n",
    "                s_exp_batch,s_next_exp_batch,a_exp_batch,r_exp_batch,d_exp_batch = buffer.get_batch(batch_size)\n",
    "                history = agent.update_er_tn(s_exp_batch,a_exp_batch,s_next_exp_batch,r_exp_batch,d_exp_batch)\n",
    "#                 print(\"loss = \",history.history[\"loss\"][0])\n",
    "\n",
    "                # copy weights to target_network\n",
    "                if step_count%synch_weights_freq == 0:\n",
    "                    agent.synch_weights()\n",
    "    #             print(\"trainstep success\")\n",
    "    \n",
    "#             else:\n",
    "#                 print(\"only adding to buffer\")\n",
    "\n",
    "            \n",
    "            # create reward per step\n",
    "            rewards.append(cum_reward)\n",
    "            \n",
    "            # check if done\n",
    "            if done:\n",
    "#                 print(\"done = \",done)\n",
    "                print(\"ep = \", ep_num, \"    steps = \", step,\"ep reward = \",cum_reward)\n",
    "                break\n",
    "                \n",
    "        \n",
    "            # set new state\n",
    "            s = s_next\n",
    "        \n",
    "        #\n",
    "        reward_per_step_per_ep.append(rewards)\n",
    "        cum_reward_per_ep.append(cum_reward)\n",
    "        # Decay probability of taking random action\n",
    "        if decay_epsilon:\n",
    "            epsilon = decay_eps(epsilon,eps_min)\n",
    "                \n",
    "    return (reward_per_step_per_ep,cum_reward_per_ep)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad0e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eps = 1000\n",
    "buffer_size = 10000\n",
    "synch_weights_freq = 20\n",
    "learning_rate = 0.01\n",
    "gamma = 0.995\n",
    "epsilon = 1\n",
    "batch_size = 24\n",
    "min_buff = 1000\n",
    "max_timesteps = 500\n",
    "eps_min = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265c70fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 1,370\n",
      "Trainable params: 1,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 1,370\n",
      "Trainable params: 1,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "ep =  0     steps =  9 ep reward =  10.0\n",
      "ep =  1     steps =  18 ep reward =  19.0\n",
      "ep =  2     steps =  13 ep reward =  14.0\n",
      "ep =  3     steps =  15 ep reward =  16.0\n",
      "ep =  4     steps =  14 ep reward =  15.0\n",
      "ep =  5     steps =  51 ep reward =  52.0\n",
      "ep =  6     steps =  27 ep reward =  28.0\n",
      "ep =  7     steps =  18 ep reward =  19.0\n",
      "ep =  8     steps =  17 ep reward =  18.0\n",
      "ep =  9     steps =  53 ep reward =  54.0\n",
      "ep =  10     steps =  18 ep reward =  19.0\n",
      "ep =  11     steps =  22 ep reward =  23.0\n",
      "ep =  12     steps =  10 ep reward =  11.0\n",
      "ep =  13     steps =  30 ep reward =  31.0\n",
      "ep =  14     steps =  12 ep reward =  13.0\n",
      "ep =  15     steps =  10 ep reward =  11.0\n",
      "ep =  16     steps =  88 ep reward =  89.0\n",
      "ep =  17     steps =  10 ep reward =  11.0\n",
      "ep =  18     steps =  11 ep reward =  12.0\n",
      "ep =  19     steps =  12 ep reward =  13.0\n",
      "ep =  20     steps =  20 ep reward =  21.0\n",
      "ep =  21     steps =  50 ep reward =  51.0\n",
      "ep =  22     steps =  15 ep reward =  16.0\n",
      "ep =  23     steps =  9 ep reward =  10.0\n",
      "ep =  24     steps =  9 ep reward =  10.0\n",
      "ep =  25     steps =  9 ep reward =  10.0\n",
      "ep =  26     steps =  8 ep reward =  9.0\n",
      "ep =  27     steps =  10 ep reward =  11.0\n",
      "ep =  28     steps =  9 ep reward =  10.0\n",
      "ep =  29     steps =  76 ep reward =  77.0\n",
      "ep =  30     steps =  10 ep reward =  11.0\n",
      "ep =  31     steps =  9 ep reward =  10.0\n",
      "ep =  32     steps =  19 ep reward =  20.0\n",
      "ep =  33     steps =  9 ep reward =  10.0\n",
      "ep =  34     steps =  19 ep reward =  20.0\n",
      "ep =  35     steps =  13 ep reward =  14.0\n",
      "ep =  36     steps =  9 ep reward =  10.0\n",
      "ep =  37     steps =  8 ep reward =  9.0\n",
      "ep =  38     steps =  30 ep reward =  31.0\n",
      "ep =  39     steps =  103 ep reward =  104.0\n",
      "ep =  40     steps =  9 ep reward =  10.0\n",
      "1000\n",
      "weights synched\n",
      "ep =  41     steps =  61 ep reward =  62.0\n",
      "weights synched\n",
      "ep =  42     steps =  10 ep reward =  11.0\n",
      "ep =  43     steps =  13 ep reward =  14.0\n",
      "weights synched\n",
      "ep =  44     steps =  10 ep reward =  11.0\n",
      "weights synched\n",
      "ep =  45     steps =  11 ep reward =  12.0\n",
      "weights synched\n",
      "ep =  46     steps =  17 ep reward =  18.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  47     steps =  93 ep reward =  94.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  48     steps =  49 ep reward =  50.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  49     steps =  74 ep reward =  75.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  50     steps =  107 ep reward =  108.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  51     steps =  81 ep reward =  82.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  52     steps =  163 ep reward =  164.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  53     steps =  63 ep reward =  64.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  54     steps =  250 ep reward =  251.0\n",
      "weights synched\n",
      "2000\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  55     steps =  124 ep reward =  125.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  56     steps =  125 ep reward =  126.0\n",
      "weights synched\n",
      "ep =  57     steps =  12 ep reward =  13.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  58     steps =  121 ep reward =  122.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  59     steps =  131 ep reward =  132.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  60     steps =  95 ep reward =  96.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  61     steps =  146 ep reward =  147.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  62     steps =  126 ep reward =  127.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  63     steps =  65 ep reward =  66.0\n",
      "weights synched\n",
      "ep =  64     steps =  36 ep reward =  37.0\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  65     steps =  27 ep reward =  28.0\n",
      "3000\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  66     steps =  137 ep reward =  138.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  67     steps =  153 ep reward =  154.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  68     steps =  182 ep reward =  183.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  69     steps =  91 ep reward =  92.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  70     steps =  98 ep reward =  99.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  71     steps =  62 ep reward =  63.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  72     steps =  141 ep reward =  142.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "4000\n",
      "weights synched\n",
      "ep =  73     steps =  158 ep reward =  159.0\n",
      "weights synched\n",
      "ep =  74     steps =  18 ep reward =  19.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  75     steps =  104 ep reward =  105.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  76     steps =  157 ep reward =  158.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  77     steps =  375 ep reward =  376.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  78     steps =  125 ep reward =  126.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "5000\n",
      "weights synched\n",
      "ep =  79     steps =  205 ep reward =  206.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  80     steps =  499 ep reward =  500.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  81     steps =  94 ep reward =  95.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  82     steps =  221 ep reward =  222.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "6000\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  83     steps =  499 ep reward =  500.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  84     steps =  259 ep reward =  260.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  85     steps =  233 ep reward =  234.0\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  86     steps =  21 ep reward =  22.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "7000\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  87     steps =  390 ep reward =  391.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  88     steps =  129 ep reward =  130.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  89     steps =  153 ep reward =  154.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  90     steps =  92 ep reward =  93.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  91     steps =  167 ep reward =  168.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  92     steps =  210 ep reward =  211.0\n",
      "8000\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  93     steps =  227 ep reward =  228.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  94     steps =  296 ep reward =  297.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  95     steps =  183 ep reward =  184.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "9000\n",
      "weights synched\n",
      "ep =  96     steps =  305 ep reward =  306.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  97     steps =  119 ep reward =  120.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  98     steps =  114 ep reward =  115.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  99     steps =  126 ep reward =  127.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  100     steps =  378 ep reward =  379.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  101     steps =  183 ep reward =  184.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  102     steps =  210 ep reward =  211.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  103     steps =  79 ep reward =  80.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  104     steps =  499 ep reward =  500.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  105     steps =  277 ep reward =  278.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  106     steps =  153 ep reward =  154.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  107     steps =  176 ep reward =  177.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  108     steps =  88 ep reward =  89.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  109     steps =  93 ep reward =  94.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  110     steps =  361 ep reward =  362.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  111     steps =  152 ep reward =  153.0\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  112     steps =  45 ep reward =  46.0\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  113     steps =  45 ep reward =  46.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  114     steps =  64 ep reward =  65.0\n",
      "ep =  115     steps =  12 ep reward =  13.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  116     steps =  192 ep reward =  193.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  117     steps =  171 ep reward =  172.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  118     steps =  112 ep reward =  113.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  119     steps =  86 ep reward =  87.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  120     steps =  94 ep reward =  95.0\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  121     steps =  36 ep reward =  37.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  122     steps =  73 ep reward =  74.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  123     steps =  79 ep reward =  80.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  124     steps =  71 ep reward =  72.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  125     steps =  107 ep reward =  108.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  126     steps =  74 ep reward =  75.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  127     steps =  499 ep reward =  500.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  128     steps =  499 ep reward =  500.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  129     steps =  499 ep reward =  500.0\n",
      "ep =  130     steps =  14 ep reward =  15.0\n",
      "weights synched\n",
      "ep =  131     steps =  22 ep reward =  23.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  132     steps =  499 ep reward =  500.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  133     steps =  499 ep reward =  500.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  134     steps =  499 ep reward =  500.0\n",
      "weights synched\n",
      "ep =  135     steps =  14 ep reward =  15.0\n",
      "weights synched\n",
      "ep =  136     steps =  14 ep reward =  15.0\n",
      "weights synched\n",
      "ep =  137     steps =  13 ep reward =  14.0\n",
      "ep =  138     steps =  14 ep reward =  15.0\n",
      "weights synched\n",
      "ep =  139     steps =  14 ep reward =  15.0\n",
      "weights synched\n",
      "ep =  140     steps =  12 ep reward =  13.0\n",
      "weights synched\n",
      "ep =  141     steps =  18 ep reward =  19.0\n",
      "weights synched\n",
      "ep =  142     steps =  19 ep reward =  20.0\n",
      "weights synched\n",
      "ep =  143     steps =  17 ep reward =  18.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  144     steps =  92 ep reward =  93.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  145     steps =  97 ep reward =  98.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x0000021A42333310>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 545, in __del__\n",
      "    gen_dataset_ops.delete_iterator(\n",
      "  File \"C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 1262, in delete_iterator\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  146     steps =  499 ep reward =  500.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  147     steps =  258 ep reward =  259.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  148     steps =  147 ep reward =  148.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  149     steps =  124 ep reward =  125.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  150     steps =  203 ep reward =  204.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  151     steps =  88 ep reward =  89.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  152     steps =  108 ep reward =  109.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "ep =  153     steps =  499 ep reward =  500.0\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n",
      "weights synched\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cb3f445fdb7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreward_per_step_per_ep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcum_reward_per_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_eps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_buff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_buff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-0ed2b9c4b4d7>\u001b[0m in \u001b[0;36mQlearn\u001b[1;34m(learning_rate, epsilon, buffer_size, n_eps, max_timesteps, batch_size, min_buff, decay_epsilon)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[1;31m# run a DQN training loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0ms_exp_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms_next_exp_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma_exp_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr_exp_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_exp_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_er_tn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_exp_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma_exp_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms_next_exp_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr_exp_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_exp_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;31m#                 print(\"loss = \",history.history[\"loss\"][0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b633f2f0ebd9>\u001b[0m in \u001b[0;36mupdate_er_tn\u001b[1;34m(self, batch_of_ss, batch_of_as, batch_of_sns, batch_of_rs, batch_of_ds)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m# calculate targets and assign done rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mG1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlive_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_of_ss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mcat_boa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_of_as\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mcat_inv_boa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_of_as\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1745\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1747\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    694\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 719\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3117\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3118\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3119\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3120\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   3121\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reward_per_step_per_ep,cum_reward_per_e = Qlearn(learning_rate, epsilon, buffer_size, n_eps, max_timesteps, batch_size, min_buff = min_buff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6299113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
